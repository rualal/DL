{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rualal/DL/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYZ6uijDCGVi",
        "colab_type": "text"
      },
      "source": [
        "### Install the nightly built Tensorflow 2.0 to use LayerNorm layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kElTdoawPxzb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "fe860216-4756-44d6-9028-b4ce9ce08652"
      },
      "source": [
        "# !pip install tensorflow-gpu==2.0.0-alpha\n",
        "!pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly-gpu-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190506)\n",
            "Requirement already satisfied: tb-nightly<1.15.0a0,>=1.14.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.14.0a20190506)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.1.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.9)\n",
            "Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.14.0.dev2019050600)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.11.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.33.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.16.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (0.15.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-gpu-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu-2.0-preview) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfoKFO4lCOzU",
        "colab_type": "text"
      },
      "source": [
        "### Import and create utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtTr3Pmr9dMd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "import time\n",
        "\n",
        "\n",
        "# Mode can be either 'train' or 'infer'\n",
        "# Set to 'infer' will skip the training\n",
        "MODE = 'train'\n",
        "# URL = 'http://www.manythings.org/anki/fra-eng.zip'\n",
        "URL = 'http://www.manythings.org/anki/ita-eng.zip'\n",
        "FILENAME = URL[URL.rfind('/'):]\n",
        "\n",
        "\n",
        "def maybe_download_and_read_file(url, filename):\n",
        "    \"\"\" Download and unzip training data\n",
        "    Args:\n",
        "        url: data url\n",
        "        filename: zip filename\n",
        "    \n",
        "    Returns:\n",
        "        Training data: an array containing text lines from the data\n",
        "    \"\"\"\n",
        "    if not os.path.exists(filename):\n",
        "        session = requests.Session()\n",
        "        response = session.get(url, stream=True)\n",
        "\n",
        "        CHUNK_SIZE = 32768\n",
        "        with open(filename, \"wb\") as f:\n",
        "            for chunk in response.iter_content(CHUNK_SIZE):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "\n",
        "    zipf = ZipFile(filename)\n",
        "    filename = zipf.namelist()\n",
        "    with zipf.open(filename[0]) as f:\n",
        "        lines = f.read()\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def normalize_string(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFCi9xR5CVCZ",
        "colab_type": "text"
      },
      "source": [
        "### Download and process training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYnj3KncAwro",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a021d892-b830-4f75-bae6-e5b593930bf8"
      },
      "source": [
        "lines = maybe_download_and_read_file(URL, FILENAME)\n",
        "lines = lines.decode('utf-8')\n",
        "\n",
        "raw_data = []\n",
        "for line in lines.split('\\n'):\n",
        "    raw_data.append(line.split('\\t'))\n",
        "\n",
        "print(raw_data[-5:])\n",
        "# The last element is empty, so omit it\n",
        "raw_data = raw_data[:-1]\n",
        "\n",
        "\n",
        "\"\"\"## Preprocessing\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "raw_data_en, raw_data_fr = list(zip(*raw_data))\n",
        "raw_data_en = [normalize_string(data) for data in raw_data_en]\n",
        "raw_data_fr_in = ['<start> ' + normalize_string(data) for data in raw_data_fr]\n",
        "raw_data_fr_out = [normalize_string(data) + ' <end>' for data in raw_data_fr]\n",
        "\n",
        "\"\"\"## Tokenization\"\"\"\n",
        "\n",
        "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "en_tokenizer.fit_on_texts(raw_data_en)\n",
        "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
        "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,\n",
        "                                                        padding='post')\n",
        "\n",
        "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "fr_tokenizer.fit_on_texts(raw_data_fr_in)\n",
        "fr_tokenizer.fit_on_texts(raw_data_fr_out)\n",
        "data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)\n",
        "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in,\n",
        "                                                           padding='post')\n",
        "\n",
        "data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)\n",
        "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out,\n",
        "                                                            padding='post')\n",
        "\n",
        "\"\"\"## Create tf.data.Dataset object\"\"\"\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (data_en, data_fr_in, data_fr_out))\n",
        "dataset = dataset.shuffle(len(data_en)).batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.', 'Se vuoi sembrare un madrelingua, devi essere disposto a esercitarti a ripetere la stessa frase più e più volte nello stesso modo in cui i suonatori di banjo praticano ripetutamente lo stesso fraseggio fino a che non riescono a suonarlo correttamente e al tempo desiderato.'], ['If you want to sound like a native speaker, you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo.', 'Se vuoi sembrare un madrelingua, devi essere disposto a esercitarti a ripetere la stessa frase più e più volte nello stesso modo in cui i suonatori di banjo praticano ripetutamente la stessa frase fino a che non riescono a suonarla correttamente e al tempo desiderato.'], [\"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\", 'Se qualcuno che non conosce il tuo background dice che sembri un madrelingua, significa che probabilmente ha notato qualcosa sul tuo modo di parlare che ha fatto capire che non eri un madrelingua. In altre parole, non sembri davvero un madrelingua.'], ['It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.', 'Può essere impossibile avere un corpus completamente libero da errori per via di questo tipo di impegno collaborativo. Ciononostante, se incoraggiamo i membri a contribuire con delle frasi nelle loro lingue piuttosto che sperimentare le lingue che stanno imparando, potremmo essere in grado di minimizzare gli errori.'], ['']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1RmU-u7CYwQ",
        "colab_type": "text"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg7_8DsDAZoc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "009992f2-d7ff-47bf-e423-f2d64ec433d4"
      },
      "source": [
        "\"\"\"## Create the Positional Embedding\"\"\"\n",
        "\n",
        "\n",
        "def positional_encoding(pos, model_size):\n",
        "    \"\"\" Compute positional encoding for a particular position\n",
        "    Args:\n",
        "        pos: position of a token in the sequence\n",
        "        model_size: depth size of the model\n",
        "    \n",
        "    Returns:\n",
        "        The positional encoding for the given token\n",
        "    \"\"\"\n",
        "    PE = np.zeros((1, model_size))\n",
        "    for i in range(model_size):\n",
        "        if i % 2 == 0:\n",
        "            PE[:, i] = np.sin(pos / 10000 ** (i / model_size))\n",
        "        else:\n",
        "            PE[:, i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n",
        "    return PE\n",
        "\n",
        "max_length = max(len(data_en[0]), len(data_fr_in[0]))\n",
        "MODEL_SIZE = 128\n",
        "\n",
        "pes = []\n",
        "for i in range(max_length):\n",
        "    pes.append(positional_encoding(i, MODEL_SIZE))\n",
        "\n",
        "pes = np.concatenate(pes, axis=0)\n",
        "pes = tf.constant(pes, dtype=tf.float32)\n",
        "\n",
        "\n",
        "print(pes.shape)\n",
        "print(data_en.shape)\n",
        "print(data_fr_in.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(53, 128)\n",
            "(321433, 50)\n",
            "(321433, 53)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJPr_lqxCbvi",
        "colab_type": "text"
      },
      "source": [
        "### Multi-Head Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgug1s0wAeAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## Create the Multihead Attention layer\"\"\"\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.Model):\n",
        "    \"\"\" Class for Multi-Head Attention layer\n",
        "    Attributes:\n",
        "        key_size: d_key in the paper\n",
        "        h: number of attention heads\n",
        "        wq: the Linear layer for Q\n",
        "        wk: the Linear layer for K\n",
        "        wv: the Linear layer for V\n",
        "        wo: the Linear layer for the output\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size, h):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.key_size = model_size // h\n",
        "        self.h = h\n",
        "        self.wq = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
        "        self.wk = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
        "        self.wv = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(value_size) for _ in range(h)]\n",
        "        self.wo = tf.keras.layers.Dense(model_size)\n",
        "\n",
        "    def call(self, query, value, mask=None):\n",
        "        \"\"\" The forward pass for Multi-Head Attention layer\n",
        "        Args:\n",
        "            query: the Q matrix\n",
        "            value: the V matrix, acts as V and K\n",
        "            mask: mask to filter out unwanted tokens\n",
        "                  - zero mask: mask for padded tokens\n",
        "                  - right-side mask: mask to prevent attention towards tokens on the right-hand side\n",
        "        \n",
        "        Returns:\n",
        "            The concatenated context vector\n",
        "            The alignment (attention) vectors of all heads\n",
        "        \"\"\"\n",
        "        # query has shape (batch, query_len, model_size)\n",
        "        # value has shape (batch, value_len, model_size)\n",
        "        query = self.wq(query)\n",
        "        key = self.wk(value)\n",
        "        value = self.wv(value)\n",
        "        \n",
        "        # Split matrices for multi-heads attention\n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        # Originally, query has shape (batch, query_len, model_size)\n",
        "        # We need to reshape to (batch, query_len, h, key_size)\n",
        "        query = tf.reshape(query, [batch_size, -1, self.h, self.key_size])\n",
        "        # In order to compute matmul, the dimensions must be transposed to (batch, h, query_len, key_size)\n",
        "        query = tf.transpose(query, [0, 2, 1, 3])\n",
        "        \n",
        "        # Do the same for key and value\n",
        "        key = tf.reshape(key, [batch_size, -1, self.h, self.key_size])\n",
        "        key = tf.transpose(key, [0, 2, 1, 3])\n",
        "        value = tf.reshape(value, [batch_size, -1, self.h, self.key_size])\n",
        "        value = tf.transpose(value, [0, 2, 1, 3])\n",
        "        \n",
        "        # Compute the dot score\n",
        "        # and divide the score by square root of key_size (as stated in paper)\n",
        "        # (must convert key_size to float32 otherwise an error would occur)\n",
        "        score = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(tf.dtypes.cast(self.key_size, dtype=tf.float32))\n",
        "        # score will have shape of (batch, h, query_len, value_len)\n",
        "        \n",
        "        # Mask out the score if a mask is provided\n",
        "        # There are two types of mask:\n",
        "        # - Padding mask (batch, 1, 1, value_len): to prevent attention being drawn to padded token (i.e. 0)\n",
        "        # - Look-left mask (batch, 1, query_len, value_len): to prevent decoder to draw attention to tokens to the right\n",
        "        if mask is not None:\n",
        "            score *= mask\n",
        "\n",
        "            # We want the masked out values to be zeros when applying softmax\n",
        "            # One way to accomplish that is assign them to a very large negative value\n",
        "            score = tf.where(tf.equal(score, 0), tf.ones_like(score) * -1e9, score)\n",
        "        \n",
        "        # Alignment vector: (batch, h, query_len, value_len)\n",
        "        alignment = tf.nn.softmax(score, axis=-1)\n",
        "        \n",
        "        # Context vector: (batch, h, query_len, key_size)\n",
        "        context = tf.matmul(alignment, value)\n",
        "        \n",
        "        # Finally, do the opposite to have a tensor of shape (batch, query_len, model_size)\n",
        "        context = tf.transpose(context, [0, 2, 1, 3])\n",
        "        context = tf.reshape(context, [batch_size, -1, self.key_size * self.h])\n",
        "        \n",
        "        # Apply one last full connected layer (WO)\n",
        "        heads = self.wo(context)\n",
        "        \n",
        "        return heads, alignment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjdTPndICfEu",
        "colab_type": "text"
      },
      "source": [
        "### The Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP3f3nloAg5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"## Create the Encoder\"\"\"\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    \"\"\" Class for the Encoder\n",
        "    Args:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
        "        h: number of attention heads\n",
        "        embedding: Embedding layer\n",
        "        embedding_dropout: Dropout layer for Embedding\n",
        "        attention: array of Multi-Head Attention layers\n",
        "        attention_dropout: array of Dropout layers for Multi-Head Attention\n",
        "        attention_norm: array of LayerNorm layers for Multi-Head Attention\n",
        "        dense_1: array of first Dense layers for FFN\n",
        "        dense_2: array of second Dense layers for FFN\n",
        "        ffn_dropout: array of Dropout layers for FFN\n",
        "        ffn_norm: array of LayerNorm layers for FFN\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
        "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.attention = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "\n",
        "        self.attention_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense_1 = [tf.keras.layers.Dense(\n",
        "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(\n",
        "            model_size) for _ in range(num_layers)]\n",
        "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self, sequence, training=True, encoder_mask=None):\n",
        "        \"\"\" Forward pass for the Encoder\n",
        "        Args:\n",
        "            sequence: source input sequences\n",
        "            training: whether training or not (for Dropout)\n",
        "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
        "        \n",
        "        Returns:\n",
        "            The output of the Encoder (batch_size, length, model_size)\n",
        "            The alignment (attention) vectors for all layers\n",
        "        \"\"\"\n",
        "        embed_out = self.embedding(sequence)\n",
        "\n",
        "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "        embed_out += pes[:sequence.shape[1], :]\n",
        "        embed_out = self.embedding_dropout(embed_out)\n",
        "\n",
        "        sub_in = embed_out\n",
        "        alignments = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            sub_out, alignment = self.attention[i](sub_in, sub_in, encoder_mask)\n",
        "            sub_out = self.attention_dropout[i](sub_out, training=training)\n",
        "            sub_out = sub_in + sub_out\n",
        "            sub_out = self.attention_norm[i](sub_out)\n",
        "            \n",
        "            alignments.append(alignment)\n",
        "            ffn_in = sub_out\n",
        "\n",
        "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
        "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
        "            ffn_out = ffn_in + ffn_out\n",
        "            ffn_out = self.ffn_norm[i](ffn_out)\n",
        "\n",
        "            sub_in = ffn_out\n",
        "\n",
        "        return ffn_out, alignments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pxeATn-Cxzv",
        "colab_type": "text"
      },
      "source": [
        "### Create an instance of the Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-iSrjyYA8ws",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5c903493-cda3-4f4a-ee03-e0be6b7b71a4"
      },
      "source": [
        "H = 8\n",
        "NUM_LAYERS = 4\n",
        "vocab_size = len(en_tokenizer.word_index) + 1\n",
        "encoder = Encoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
        "print(vocab_size)\n",
        "sequence_in = tf.constant([[1, 2, 3, 0, 0]])\n",
        "encoder_output, _ = encoder(sequence_in)\n",
        "encoder_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12807\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 5, 128])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XShJ6Z0mCvyo",
        "colab_type": "text"
      },
      "source": [
        "### The Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH6eFjbmBAfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    \"\"\" Class for the Decoder\n",
        "    Args:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
        "        h: number of attention heads\n",
        "        embedding: Embedding layer\n",
        "        embedding_dropout: Dropout layer for Embedding\n",
        "        attention_bot: array of bottom Multi-Head Attention layers (self attention)\n",
        "        attention_bot_dropout: array of Dropout layers for bottom Multi-Head Attention\n",
        "        attention_bot_norm: array of LayerNorm layers for bottom Multi-Head Attention\n",
        "        attention_mid: array of middle Multi-Head Attention layers\n",
        "        attention_mid_dropout: array of Dropout layers for middle Multi-Head Attention\n",
        "        attention_mid_norm: array of LayerNorm layers for middle Multi-Head Attention\n",
        "        dense_1: array of first Dense layers for FFN\n",
        "        dense_2: array of second Dense layers for FFN\n",
        "        ffn_dropout: array of Dropout layers for FFN\n",
        "        ffn_norm: array of LayerNorm layers for FFN\n",
        "        dense: Dense layer to compute final output\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
        "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.attention_bot = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_bot_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.attention_bot_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "        self.attention_mid = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_mid_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.attention_mid_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense_1 = [tf.keras.layers.Dense(\n",
        "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(\n",
        "            model_size) for _ in range(num_layers)]\n",
        "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, sequence, encoder_output, training=True, encoder_mask=None):\n",
        "        \"\"\" Forward pass for the Decoder\n",
        "        Args:\n",
        "            sequence: source input sequences\n",
        "            encoder_output: output of the Encoder (for computing middle attention)\n",
        "            training: whether training or not (for Dropout)\n",
        "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
        "        \n",
        "        Returns:\n",
        "            The output of the Encoder (batch_size, length, model_size)\n",
        "            The bottom alignment (attention) vectors for all layers\n",
        "            The middle alignment (attention) vectors for all layers\n",
        "        \"\"\"\n",
        "        # EMBEDDING AND POSITIONAL EMBEDDING\n",
        "        embed_out = self.embedding(sequence)\n",
        "\n",
        "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "        embed_out += pes[:sequence.shape[1], :]\n",
        "        embed_out = self.embedding_dropout(embed_out)\n",
        "\n",
        "        bot_sub_in = embed_out\n",
        "        bot_alignments = []\n",
        "        mid_alignments = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # BOTTOM MULTIHEAD SUB LAYER\n",
        "            seq_len = bot_sub_in.shape[1]\n",
        "\n",
        "            if training:\n",
        "                mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "            else:\n",
        "                mask = None\n",
        "            bot_sub_out, bot_alignment = self.attention_bot[i](bot_sub_in, bot_sub_in, mask)\n",
        "            bot_sub_out = self.attention_bot_dropout[i](bot_sub_out, training=training)\n",
        "            bot_sub_out = bot_sub_in + bot_sub_out\n",
        "            bot_sub_out = self.attention_bot_norm[i](bot_sub_out)\n",
        "            \n",
        "            bot_alignments.append(bot_alignment)\n",
        "\n",
        "            # MIDDLE MULTIHEAD SUB LAYER\n",
        "            mid_sub_in = bot_sub_out\n",
        "\n",
        "            mid_sub_out, mid_alignment = self.attention_mid[i](\n",
        "                mid_sub_in, encoder_output, encoder_mask)\n",
        "            mid_sub_out = self.attention_mid_dropout[i](mid_sub_out, training=training)\n",
        "            mid_sub_out = mid_sub_out + mid_sub_in\n",
        "            mid_sub_out = self.attention_mid_norm[i](mid_sub_out)\n",
        "            \n",
        "            mid_alignments.append(mid_alignment)\n",
        "\n",
        "            # FFN\n",
        "            ffn_in = mid_sub_out\n",
        "\n",
        "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
        "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
        "            ffn_out = ffn_out + ffn_in\n",
        "            ffn_out = self.ffn_norm[i](ffn_out)\n",
        "\n",
        "            bot_sub_in = ffn_out\n",
        "\n",
        "        logits = self.dense(ffn_out)\n",
        "\n",
        "        return logits, bot_alignments, mid_alignments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19DfjeaxCilt",
        "colab_type": "text"
      },
      "source": [
        "### Create an instance of the Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDLPEA30BDjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "77ed6ce5-2d1f-48d9-f3d3-de58f02ce96f"
      },
      "source": [
        "vocab_size = len(fr_tokenizer.word_index) + 1\n",
        "decoder = Decoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
        "\n",
        "sequence_in = tf.constant([[14, 24, 36, 0, 0]])\n",
        "decoder_output, _, _ = decoder(sequence_in, encoder_output)\n",
        "decoder_output.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 5, 25376])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBYEswwMC01W",
        "colab_type": "text"
      },
      "source": [
        "### Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CmFGkJ2BMOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True)\n",
        "\n",
        "\n",
        "def loss_func(targets, logits):\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjVxUv2tC3MM",
        "colab_type": "text"
      },
      "source": [
        "### Learning rate scheduling and optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwgwnb-3BNhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WarmupThenDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\" Learning schedule for training the Transformer\n",
        "    Attributes:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        warmup_steps: number of warmup steps at the beginning\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size, warmup_steps=4000):\n",
        "        super(WarmupThenDecaySchedule, self).__init__()\n",
        "\n",
        "        self.model_size = model_size\n",
        "        self.model_size = tf.cast(self.model_size, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step_term = tf.math.rsqrt(step)\n",
        "        warmup_term = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.model_size) * tf.math.minimum(step_term, warmup_term)\n",
        "\n",
        "\n",
        "lr = WarmupThenDecaySchedule(MODEL_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(lr,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMvgwyoDC9ks",
        "colab_type": "text"
      },
      "source": [
        "### The predict function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRqSWlTcBPq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(test_source_text=None):\n",
        "    \"\"\" Predict the output sentence for a given input sentence\n",
        "    Args:\n",
        "        test_source_text: input sentence (raw string)\n",
        "    \n",
        "    Returns:\n",
        "        The encoder's attention vectors\n",
        "        The decoder's bottom attention vectors\n",
        "        The decoder's middle attention vectors\n",
        "        The input string array (input sentence split by ' ')\n",
        "        The output string array\n",
        "    \"\"\"\n",
        "    if test_source_text is None:\n",
        "        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
        "    print(test_source_text)\n",
        "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
        "    print(test_source_seq)\n",
        "\n",
        "    en_output, en_alignments = encoder(tf.constant(test_source_seq), training=False)\n",
        "\n",
        "    de_input = tf.constant(\n",
        "        [[fr_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
        "\n",
        "    out_words = []\n",
        "\n",
        "    while True:\n",
        "        de_output, de_bot_alignments, de_mid_alignments = decoder(de_input, en_output, training=False)\n",
        "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
        "        out_words.append(fr_tokenizer.index_word[new_word.numpy()[0][0]])\n",
        "\n",
        "        # Transformer doesn't have sequential mechanism (i.e. states)\n",
        "        # so we have to add the last predicted word to create a new input sequence\n",
        "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
        "\n",
        "        # TODO: get a nicer constraint for the sequence length!\n",
        "        if out_words[-1] == '<end>' or len(out_words) >= 14:\n",
        "            break\n",
        "\n",
        "    print(' '.join(out_words))\n",
        "    return en_alignments, de_bot_alignments, de_mid_alignments, test_source_text.split(' '), out_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn4MBXCOC_jB",
        "colab_type": "text"
      },
      "source": [
        "### The train_step function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiUJ7_Y5BSAz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(source_seq, target_seq_in, target_seq_out):\n",
        "    \"\"\" Execute one training step (forward pass + backward pass)\n",
        "    Args:\n",
        "        source_seq: source sequences\n",
        "        target_seq_in: input target sequences (<start> + ...)\n",
        "        target_seq_out: output target sequences (... + <end>)\n",
        "    \n",
        "    Returns:\n",
        "        The loss value of the current pass\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_mask = 1 - tf.cast(tf.equal(source_seq, 0), dtype=tf.float32)\n",
        "        # encoder_mask has shape (batch_size, source_len)\n",
        "        # we need to add two more dimensions in between\n",
        "        # to make it broadcastable when computing attention heads\n",
        "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
        "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
        "        encoder_output, _ = encoder(source_seq, encoder_mask=encoder_mask)\n",
        "\n",
        "        decoder_output, _, _ = decoder(\n",
        "            target_seq_in, encoder_output, encoder_mask=encoder_mask)\n",
        "\n",
        "        loss = loss_func(target_seq_out, decoder_output)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5H0UbfNDB4S",
        "colab_type": "text"
      },
      "source": [
        "### The training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyXdcoI3BTz6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 15079
        },
        "outputId": "fd7026b7-5169-4c5b-ed66-3c3349c40bd8"
      },
      "source": [
        "NUM_EPOCHS = 20\n",
        " \n",
        "starttime = time.time()\n",
        "for e in range(NUM_EPOCHS):\n",
        "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "        loss = train_step(source_seq, target_seq_in,\n",
        "                          target_seq_out)\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f} Elapsed time {:.2f}s'.format(\n",
        "                e + 1, batch, loss.numpy(), time.time() - starttime))\n",
        "            starttime = time.time()\n",
        " \n",
        "    try:\n",
        "        predict()\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        continue"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.3975 Elapsed time 28.12s\n",
            "Epoch 1 Batch 100 Loss 1.2918 Elapsed time 13.65s\n",
            "Epoch 1 Batch 200 Loss 1.2353 Elapsed time 13.66s\n",
            "Epoch 1 Batch 300 Loss 1.0152 Elapsed time 13.75s\n",
            "Epoch 1 Batch 400 Loss 0.8643 Elapsed time 13.73s\n",
            "Epoch 1 Batch 500 Loss 0.7901 Elapsed time 13.69s\n",
            "Epoch 1 Batch 600 Loss 0.7102 Elapsed time 13.81s\n",
            "Epoch 1 Batch 700 Loss 0.6395 Elapsed time 13.68s\n",
            "Epoch 1 Batch 800 Loss 0.6652 Elapsed time 13.74s\n",
            "Epoch 1 Batch 900 Loss 0.6693 Elapsed time 13.69s\n",
            "Epoch 1 Batch 1000 Loss 0.5680 Elapsed time 13.73s\n",
            "Epoch 1 Batch 1100 Loss 0.5516 Elapsed time 13.67s\n",
            "Epoch 1 Batch 1200 Loss 0.5000 Elapsed time 13.82s\n",
            "Epoch 1 Batch 1300 Loss 0.5309 Elapsed time 14.09s\n",
            "Epoch 1 Batch 1400 Loss 0.4815 Elapsed time 13.79s\n",
            "Epoch 1 Batch 1500 Loss 0.5278 Elapsed time 13.77s\n",
            "Epoch 1 Batch 1600 Loss 0.4500 Elapsed time 13.68s\n",
            "Epoch 1 Batch 1700 Loss 0.4448 Elapsed time 13.71s\n",
            "Epoch 1 Batch 1800 Loss 0.5055 Elapsed time 13.87s\n",
            "Epoch 1 Batch 1900 Loss 0.4272 Elapsed time 13.71s\n",
            "Epoch 1 Batch 2000 Loss 0.4311 Elapsed time 13.66s\n",
            "Epoch 1 Batch 2100 Loss 0.4148 Elapsed time 13.67s\n",
            "Epoch 1 Batch 2200 Loss 0.3438 Elapsed time 13.69s\n",
            "Epoch 1 Batch 2300 Loss 0.3452 Elapsed time 13.75s\n",
            "Epoch 1 Batch 2400 Loss 0.3253 Elapsed time 13.89s\n",
            "Epoch 1 Batch 2500 Loss 0.3214 Elapsed time 13.74s\n",
            "Epoch 1 Batch 2600 Loss 0.3081 Elapsed time 13.66s\n",
            "Epoch 1 Batch 2700 Loss 0.2680 Elapsed time 13.67s\n",
            "Epoch 1 Batch 2800 Loss 0.2767 Elapsed time 13.73s\n",
            "Epoch 1 Batch 2900 Loss 0.3032 Elapsed time 13.69s\n",
            "Epoch 1 Batch 3000 Loss 0.3277 Elapsed time 13.84s\n",
            "Epoch 1 Batch 3100 Loss 0.2439 Elapsed time 13.77s\n",
            "Epoch 1 Batch 3200 Loss 0.2663 Elapsed time 13.66s\n",
            "Epoch 1 Batch 3300 Loss 0.3848 Elapsed time 13.70s\n",
            "Epoch 1 Batch 3400 Loss 0.3359 Elapsed time 13.67s\n",
            "Epoch 1 Batch 3500 Loss 0.2267 Elapsed time 13.82s\n",
            "Epoch 1 Batch 3600 Loss 0.2302 Elapsed time 13.93s\n",
            "Epoch 1 Batch 3700 Loss 0.2490 Elapsed time 13.75s\n",
            "Epoch 1 Batch 3800 Loss 0.2421 Elapsed time 13.71s\n",
            "Epoch 1 Batch 3900 Loss 0.2478 Elapsed time 13.76s\n",
            "Epoch 1 Batch 4000 Loss 0.2390 Elapsed time 13.71s\n",
            "Epoch 1 Batch 4100 Loss 0.2636 Elapsed time 13.81s\n",
            "Epoch 1 Batch 4200 Loss 0.2610 Elapsed time 13.72s\n",
            "Epoch 1 Batch 4300 Loss 0.2237 Elapsed time 13.77s\n",
            "Epoch 1 Batch 4400 Loss 0.2546 Elapsed time 13.75s\n",
            "Epoch 1 Batch 4500 Loss 0.2419 Elapsed time 13.72s\n",
            "Epoch 1 Batch 4600 Loss 0.2069 Elapsed time 13.69s\n",
            "Epoch 1 Batch 4700 Loss 0.2185 Elapsed time 13.85s\n",
            "Epoch 1 Batch 4800 Loss 0.1931 Elapsed time 13.73s\n",
            "Epoch 1 Batch 4900 Loss 0.2541 Elapsed time 13.77s\n",
            "Epoch 1 Batch 5000 Loss 0.2287 Elapsed time 13.75s\n",
            "I hope it s not Tom .\n",
            "[[2, 204, 11, 13, 36, 4, 1]]\n",
            "spero che non sia tom . <end>\n",
            "Epoch 2 Batch 0 Loss 0.2021 Elapsed time 21.15s\n",
            "Epoch 2 Batch 100 Loss 0.1603 Elapsed time 13.75s\n",
            "Epoch 2 Batch 200 Loss 0.1997 Elapsed time 13.81s\n",
            "Epoch 2 Batch 300 Loss 0.2306 Elapsed time 13.78s\n",
            "Epoch 2 Batch 400 Loss 0.1726 Elapsed time 13.68s\n",
            "Epoch 2 Batch 500 Loss 0.2018 Elapsed time 13.62s\n",
            "Epoch 2 Batch 600 Loss 0.2205 Elapsed time 13.84s\n",
            "Epoch 2 Batch 700 Loss 0.1926 Elapsed time 13.95s\n",
            "Epoch 2 Batch 800 Loss 0.2217 Elapsed time 13.73s\n",
            "Epoch 2 Batch 900 Loss 0.1956 Elapsed time 13.64s\n",
            "Epoch 2 Batch 1000 Loss 0.1837 Elapsed time 13.65s\n",
            "Epoch 2 Batch 1100 Loss 0.2437 Elapsed time 13.59s\n",
            "Epoch 2 Batch 1200 Loss 0.1872 Elapsed time 13.69s\n",
            "Epoch 2 Batch 1300 Loss 0.2104 Elapsed time 13.84s\n",
            "Epoch 2 Batch 1400 Loss 0.2005 Elapsed time 13.71s\n",
            "Epoch 2 Batch 1500 Loss 0.2307 Elapsed time 13.69s\n",
            "Epoch 2 Batch 1600 Loss 0.1925 Elapsed time 13.69s\n",
            "Epoch 2 Batch 1700 Loss 0.1800 Elapsed time 13.70s\n",
            "Epoch 2 Batch 1800 Loss 0.2582 Elapsed time 13.64s\n",
            "Epoch 2 Batch 1900 Loss 0.2090 Elapsed time 13.87s\n",
            "Epoch 2 Batch 2000 Loss 0.1830 Elapsed time 13.71s\n",
            "Epoch 2 Batch 2100 Loss 0.2075 Elapsed time 13.70s\n",
            "Epoch 2 Batch 2200 Loss 0.1537 Elapsed time 13.71s\n",
            "Epoch 2 Batch 2300 Loss 0.1767 Elapsed time 13.66s\n",
            "Epoch 2 Batch 2400 Loss 0.1663 Elapsed time 13.79s\n",
            "Epoch 2 Batch 2500 Loss 0.1985 Elapsed time 13.82s\n",
            "Epoch 2 Batch 2600 Loss 0.1696 Elapsed time 13.66s\n",
            "Epoch 2 Batch 2700 Loss 0.1395 Elapsed time 13.64s\n",
            "Epoch 2 Batch 2800 Loss 0.1541 Elapsed time 13.74s\n",
            "Epoch 2 Batch 2900 Loss 0.1857 Elapsed time 13.91s\n",
            "Epoch 2 Batch 3000 Loss 0.2141 Elapsed time 13.79s\n",
            "Epoch 2 Batch 3100 Loss 0.1254 Elapsed time 13.81s\n",
            "Epoch 2 Batch 3200 Loss 0.1818 Elapsed time 13.71s\n",
            "Epoch 2 Batch 3300 Loss 0.2724 Elapsed time 13.63s\n",
            "Epoch 2 Batch 3400 Loss 0.2299 Elapsed time 13.60s\n",
            "Epoch 2 Batch 3500 Loss 0.1556 Elapsed time 13.62s\n",
            "Epoch 2 Batch 3600 Loss 0.1488 Elapsed time 13.63s\n",
            "Epoch 2 Batch 3700 Loss 0.1588 Elapsed time 13.77s\n",
            "Epoch 2 Batch 3800 Loss 0.1527 Elapsed time 13.62s\n",
            "Epoch 2 Batch 3900 Loss 0.1520 Elapsed time 13.58s\n",
            "Epoch 2 Batch 4000 Loss 0.1786 Elapsed time 13.60s\n",
            "Epoch 2 Batch 4100 Loss 0.1827 Elapsed time 13.63s\n",
            "Epoch 2 Batch 4200 Loss 0.1926 Elapsed time 13.68s\n",
            "Epoch 2 Batch 4300 Loss 0.1561 Elapsed time 13.71s\n",
            "Epoch 2 Batch 4400 Loss 0.1608 Elapsed time 13.65s\n",
            "Epoch 2 Batch 4500 Loss 0.1727 Elapsed time 13.59s\n",
            "Epoch 2 Batch 4600 Loss 0.1469 Elapsed time 13.66s\n",
            "Epoch 2 Batch 4700 Loss 0.1556 Elapsed time 13.61s\n",
            "Epoch 2 Batch 4800 Loss 0.1446 Elapsed time 13.68s\n",
            "Epoch 2 Batch 4900 Loss 0.1854 Elapsed time 13.74s\n",
            "Epoch 2 Batch 5000 Loss 0.1742 Elapsed time 13.70s\n",
            "I m impressed with your French .\n",
            "[[2, 21, 918, 35, 32, 99, 1]]\n",
            "sono veloce del tuo francese . <end>\n",
            "Epoch 3 Batch 0 Loss 0.1536 Elapsed time 4.17s\n",
            "Epoch 3 Batch 100 Loss 0.1233 Elapsed time 13.70s\n",
            "Epoch 3 Batch 200 Loss 0.1577 Elapsed time 13.82s\n",
            "Epoch 3 Batch 300 Loss 0.1988 Elapsed time 13.63s\n",
            "Epoch 3 Batch 400 Loss 0.1334 Elapsed time 13.85s\n",
            "Epoch 3 Batch 500 Loss 0.1568 Elapsed time 13.61s\n",
            "Epoch 3 Batch 600 Loss 0.1676 Elapsed time 13.64s\n",
            "Epoch 3 Batch 700 Loss 0.1629 Elapsed time 13.60s\n",
            "Epoch 3 Batch 800 Loss 0.1674 Elapsed time 13.62s\n",
            "Epoch 3 Batch 900 Loss 0.1734 Elapsed time 13.59s\n",
            "Epoch 3 Batch 1000 Loss 0.1375 Elapsed time 13.75s\n",
            "Epoch 3 Batch 1100 Loss 0.1914 Elapsed time 13.62s\n",
            "Epoch 3 Batch 1200 Loss 0.1523 Elapsed time 13.53s\n",
            "Epoch 3 Batch 1300 Loss 0.1664 Elapsed time 13.63s\n",
            "Epoch 3 Batch 1400 Loss 0.1517 Elapsed time 13.62s\n",
            "Epoch 3 Batch 1500 Loss 0.1974 Elapsed time 13.55s\n",
            "Epoch 3 Batch 1600 Loss 0.1617 Elapsed time 13.79s\n",
            "Epoch 3 Batch 1700 Loss 0.1581 Elapsed time 13.59s\n",
            "Epoch 3 Batch 1800 Loss 0.2170 Elapsed time 13.58s\n",
            "Epoch 3 Batch 1900 Loss 0.1774 Elapsed time 13.60s\n",
            "Epoch 3 Batch 2000 Loss 0.1551 Elapsed time 13.59s\n",
            "Epoch 3 Batch 2100 Loss 0.1807 Elapsed time 13.59s\n",
            "Epoch 3 Batch 2200 Loss 0.1381 Elapsed time 13.76s\n",
            "Epoch 3 Batch 2300 Loss 0.1430 Elapsed time 13.75s\n",
            "Epoch 3 Batch 2400 Loss 0.1480 Elapsed time 13.71s\n",
            "Epoch 3 Batch 2500 Loss 0.1733 Elapsed time 13.77s\n",
            "Epoch 3 Batch 2600 Loss 0.1478 Elapsed time 13.56s\n",
            "Epoch 3 Batch 2700 Loss 0.1210 Elapsed time 13.64s\n",
            "Epoch 3 Batch 2800 Loss 0.1218 Elapsed time 13.75s\n",
            "Epoch 3 Batch 2900 Loss 0.1546 Elapsed time 13.54s\n",
            "Epoch 3 Batch 3000 Loss 0.1939 Elapsed time 13.61s\n",
            "Epoch 3 Batch 3100 Loss 0.1146 Elapsed time 13.55s\n",
            "Epoch 3 Batch 3200 Loss 0.1696 Elapsed time 13.57s\n",
            "Epoch 3 Batch 3300 Loss 0.2259 Elapsed time 13.66s\n",
            "Epoch 3 Batch 3400 Loss 0.2125 Elapsed time 13.74s\n",
            "Epoch 3 Batch 3500 Loss 0.1366 Elapsed time 13.59s\n",
            "Epoch 3 Batch 3600 Loss 0.1350 Elapsed time 13.59s\n",
            "Epoch 3 Batch 3700 Loss 0.1449 Elapsed time 13.62s\n",
            "Epoch 3 Batch 3800 Loss 0.1287 Elapsed time 13.64s\n",
            "Epoch 3 Batch 3900 Loss 0.1379 Elapsed time 13.68s\n",
            "Epoch 3 Batch 4000 Loss 0.1555 Elapsed time 13.71s\n",
            "Epoch 3 Batch 4100 Loss 0.1671 Elapsed time 13.61s\n",
            "Epoch 3 Batch 4200 Loss 0.1713 Elapsed time 13.67s\n",
            "Epoch 3 Batch 4300 Loss 0.1427 Elapsed time 13.58s\n",
            "Epoch 3 Batch 4400 Loss 0.1448 Elapsed time 13.62s\n",
            "Epoch 3 Batch 4500 Loss 0.1575 Elapsed time 13.81s\n",
            "Epoch 3 Batch 4600 Loss 0.1226 Elapsed time 13.69s\n",
            "Epoch 3 Batch 4700 Loss 0.1423 Elapsed time 13.85s\n",
            "Epoch 3 Batch 4800 Loss 0.1278 Elapsed time 13.66s\n",
            "Epoch 3 Batch 4900 Loss 0.1679 Elapsed time 13.58s\n",
            "Epoch 3 Batch 5000 Loss 0.1517 Elapsed time 13.59s\n",
            "I think Tom is from Australia .\n",
            "[[2, 43, 4, 10, 89, 256, 1]]\n",
            "penso tom . <end>\n",
            "Epoch 4 Batch 0 Loss 0.1454 Elapsed time 3.82s\n",
            "Epoch 4 Batch 100 Loss 0.1161 Elapsed time 13.83s\n",
            "Epoch 4 Batch 200 Loss 0.1452 Elapsed time 13.62s\n",
            "Epoch 4 Batch 300 Loss 0.1857 Elapsed time 13.60s\n",
            "Epoch 4 Batch 400 Loss 0.1182 Elapsed time 13.55s\n",
            "Epoch 4 Batch 500 Loss 0.1440 Elapsed time 13.57s\n",
            "Epoch 4 Batch 600 Loss 0.1540 Elapsed time 13.66s\n",
            "Epoch 4 Batch 700 Loss 0.1605 Elapsed time 13.79s\n",
            "Epoch 4 Batch 800 Loss 0.1580 Elapsed time 13.65s\n",
            "Epoch 4 Batch 900 Loss 0.1584 Elapsed time 13.63s\n",
            "Epoch 4 Batch 1000 Loss 0.1294 Elapsed time 13.70s\n",
            "Epoch 4 Batch 1100 Loss 0.1822 Elapsed time 13.65s\n",
            "Epoch 4 Batch 1200 Loss 0.1386 Elapsed time 13.72s\n",
            "Epoch 4 Batch 1300 Loss 0.1596 Elapsed time 13.82s\n",
            "Epoch 4 Batch 1400 Loss 0.1328 Elapsed time 13.65s\n",
            "Epoch 4 Batch 1500 Loss 0.1767 Elapsed time 13.71s\n",
            "Epoch 4 Batch 1600 Loss 0.1549 Elapsed time 13.62s\n",
            "Epoch 4 Batch 1700 Loss 0.1393 Elapsed time 13.75s\n",
            "Epoch 4 Batch 1800 Loss 0.1949 Elapsed time 13.64s\n",
            "Epoch 4 Batch 1900 Loss 0.1656 Elapsed time 13.81s\n",
            "Epoch 4 Batch 2000 Loss 0.1354 Elapsed time 13.79s\n",
            "Epoch 4 Batch 2100 Loss 0.1650 Elapsed time 13.69s\n",
            "Epoch 4 Batch 2200 Loss 0.1286 Elapsed time 13.63s\n",
            "Epoch 4 Batch 2300 Loss 0.1380 Elapsed time 13.59s\n",
            "Epoch 4 Batch 2400 Loss 0.1431 Elapsed time 13.66s\n",
            "Epoch 4 Batch 2500 Loss 0.1665 Elapsed time 13.72s\n",
            "Epoch 4 Batch 2600 Loss 0.1381 Elapsed time 13.68s\n",
            "Epoch 4 Batch 2700 Loss 0.1185 Elapsed time 13.59s\n",
            "Epoch 4 Batch 2800 Loss 0.1167 Elapsed time 13.67s\n",
            "Epoch 4 Batch 2900 Loss 0.1470 Elapsed time 13.69s\n",
            "Epoch 4 Batch 3000 Loss 0.1833 Elapsed time 13.71s\n",
            "Epoch 4 Batch 3100 Loss 0.1107 Elapsed time 13.73s\n",
            "Epoch 4 Batch 3200 Loss 0.1547 Elapsed time 13.59s\n",
            "Epoch 4 Batch 3300 Loss 0.2085 Elapsed time 13.62s\n",
            "Epoch 4 Batch 3400 Loss 0.2056 Elapsed time 13.65s\n",
            "Epoch 4 Batch 3500 Loss 0.1310 Elapsed time 13.58s\n",
            "Epoch 4 Batch 3600 Loss 0.1305 Elapsed time 13.74s\n",
            "Epoch 4 Batch 3700 Loss 0.1315 Elapsed time 13.67s\n",
            "Epoch 4 Batch 3800 Loss 0.1161 Elapsed time 13.56s\n",
            "Epoch 4 Batch 3900 Loss 0.1235 Elapsed time 13.63s\n",
            "Epoch 4 Batch 4000 Loss 0.1470 Elapsed time 13.71s\n",
            "Epoch 4 Batch 4100 Loss 0.1609 Elapsed time 13.63s\n",
            "Epoch 4 Batch 4200 Loss 0.1659 Elapsed time 13.88s\n",
            "Epoch 4 Batch 4300 Loss 0.1346 Elapsed time 13.70s\n",
            "Epoch 4 Batch 4400 Loss 0.1294 Elapsed time 13.63s\n",
            "Epoch 4 Batch 4500 Loss 0.1457 Elapsed time 13.65s\n",
            "Epoch 4 Batch 4600 Loss 0.1138 Elapsed time 13.71s\n",
            "Epoch 4 Batch 4700 Loss 0.1345 Elapsed time 13.63s\n",
            "Epoch 4 Batch 4800 Loss 0.1194 Elapsed time 13.84s\n",
            "Epoch 4 Batch 4900 Loss 0.1591 Elapsed time 13.61s\n",
            "Epoch 4 Batch 5000 Loss 0.1407 Elapsed time 13.69s\n",
            "Who are these for ?\n",
            "[[74, 25, 200, 30, 6]]\n",
            "per questi ? <end>\n",
            "Epoch 5 Batch 0 Loss 0.1439 Elapsed time 3.89s\n",
            "Epoch 5 Batch 100 Loss 0.1052 Elapsed time 13.60s\n",
            "Epoch 5 Batch 200 Loss 0.1381 Elapsed time 13.65s\n",
            "Epoch 5 Batch 300 Loss 0.1766 Elapsed time 13.64s\n",
            "Epoch 5 Batch 400 Loss 0.1121 Elapsed time 13.79s\n",
            "Epoch 5 Batch 500 Loss 0.1322 Elapsed time 13.68s\n",
            "Epoch 5 Batch 600 Loss 0.1423 Elapsed time 13.60s\n",
            "Epoch 5 Batch 700 Loss 0.1534 Elapsed time 13.65s\n",
            "Epoch 5 Batch 800 Loss 0.1482 Elapsed time 13.67s\n",
            "Epoch 5 Batch 900 Loss 0.1509 Elapsed time 13.66s\n",
            "Epoch 5 Batch 1000 Loss 0.1281 Elapsed time 13.75s\n",
            "Epoch 5 Batch 1100 Loss 0.1782 Elapsed time 13.67s\n",
            "Epoch 5 Batch 1200 Loss 0.1318 Elapsed time 13.77s\n",
            "Epoch 5 Batch 1300 Loss 0.1440 Elapsed time 13.70s\n",
            "Epoch 5 Batch 1400 Loss 0.1226 Elapsed time 13.78s\n",
            "Epoch 5 Batch 1500 Loss 0.1661 Elapsed time 13.95s\n",
            "Epoch 5 Batch 1600 Loss 0.1474 Elapsed time 13.69s\n",
            "Epoch 5 Batch 1700 Loss 0.1307 Elapsed time 13.56s\n",
            "Epoch 5 Batch 1800 Loss 0.1852 Elapsed time 13.65s\n",
            "Epoch 5 Batch 1900 Loss 0.1571 Elapsed time 13.62s\n",
            "Epoch 5 Batch 2000 Loss 0.1234 Elapsed time 13.62s\n",
            "Epoch 5 Batch 2100 Loss 0.1609 Elapsed time 13.74s\n",
            "Epoch 5 Batch 2200 Loss 0.1269 Elapsed time 13.70s\n",
            "Epoch 5 Batch 2300 Loss 0.1335 Elapsed time 13.64s\n",
            "Epoch 5 Batch 2400 Loss 0.1355 Elapsed time 13.65s\n",
            "Epoch 5 Batch 2500 Loss 0.1577 Elapsed time 13.60s\n",
            "Epoch 5 Batch 2600 Loss 0.1383 Elapsed time 13.63s\n",
            "Epoch 5 Batch 2700 Loss 0.1106 Elapsed time 13.80s\n",
            "Epoch 5 Batch 2800 Loss 0.1077 Elapsed time 13.70s\n",
            "Epoch 5 Batch 2900 Loss 0.1360 Elapsed time 13.64s\n",
            "Epoch 5 Batch 3000 Loss 0.1700 Elapsed time 13.61s\n",
            "Epoch 5 Batch 3100 Loss 0.1079 Elapsed time 13.66s\n",
            "Epoch 5 Batch 3200 Loss 0.1514 Elapsed time 13.61s\n",
            "Epoch 5 Batch 3300 Loss 0.1970 Elapsed time 13.74s\n",
            "Epoch 5 Batch 3400 Loss 0.2010 Elapsed time 13.70s\n",
            "Epoch 5 Batch 3500 Loss 0.1245 Elapsed time 13.61s\n",
            "Epoch 5 Batch 3600 Loss 0.1220 Elapsed time 13.68s\n",
            "Epoch 5 Batch 3700 Loss 0.1307 Elapsed time 13.72s\n",
            "Epoch 5 Batch 3800 Loss 0.1063 Elapsed time 13.74s\n",
            "Epoch 5 Batch 3900 Loss 0.1168 Elapsed time 13.80s\n",
            "Epoch 5 Batch 4000 Loss 0.1411 Elapsed time 13.60s\n",
            "Epoch 5 Batch 4100 Loss 0.1525 Elapsed time 13.61s\n",
            "Epoch 5 Batch 4200 Loss 0.1586 Elapsed time 13.61s\n",
            "Epoch 5 Batch 4300 Loss 0.1267 Elapsed time 13.57s\n",
            "Epoch 5 Batch 4400 Loss 0.1312 Elapsed time 13.65s\n",
            "Epoch 5 Batch 4500 Loss 0.1365 Elapsed time 13.83s\n",
            "Epoch 5 Batch 4600 Loss 0.1100 Elapsed time 13.66s\n",
            "Epoch 5 Batch 4700 Loss 0.1268 Elapsed time 13.66s\n",
            "Epoch 5 Batch 4800 Loss 0.1129 Elapsed time 13.67s\n",
            "Epoch 5 Batch 4900 Loss 0.1539 Elapsed time 13.57s\n",
            "Epoch 5 Batch 5000 Loss 0.1359 Elapsed time 13.60s\n",
            "I leave that to you .\n",
            "[[2, 140, 15, 5, 3, 1]]\n",
            "te . <end>\n",
            "Epoch 6 Batch 0 Loss 0.1316 Elapsed time 3.90s\n",
            "Epoch 6 Batch 100 Loss 0.0995 Elapsed time 13.69s\n",
            "Epoch 6 Batch 200 Loss 0.1345 Elapsed time 13.63s\n",
            "Epoch 6 Batch 300 Loss 0.1669 Elapsed time 13.60s\n",
            "Epoch 6 Batch 400 Loss 0.1079 Elapsed time 13.60s\n",
            "Epoch 6 Batch 500 Loss 0.1242 Elapsed time 13.59s\n",
            "Epoch 6 Batch 600 Loss 0.1352 Elapsed time 13.79s\n",
            "Epoch 6 Batch 700 Loss 0.1491 Elapsed time 13.64s\n",
            "Epoch 6 Batch 800 Loss 0.1341 Elapsed time 13.57s\n",
            "Epoch 6 Batch 900 Loss 0.1493 Elapsed time 13.58s\n",
            "Epoch 6 Batch 1000 Loss 0.1099 Elapsed time 13.82s\n",
            "Epoch 6 Batch 1100 Loss 0.1730 Elapsed time 13.50s\n",
            "Epoch 6 Batch 1200 Loss 0.1326 Elapsed time 13.71s\n",
            "Epoch 6 Batch 1300 Loss 0.1384 Elapsed time 13.59s\n",
            "Epoch 6 Batch 1400 Loss 0.1209 Elapsed time 13.53s\n",
            "Epoch 6 Batch 1500 Loss 0.1597 Elapsed time 13.57s\n",
            "Epoch 6 Batch 1600 Loss 0.1442 Elapsed time 13.61s\n",
            "Epoch 6 Batch 1700 Loss 0.1263 Elapsed time 13.57s\n",
            "Epoch 6 Batch 1800 Loss 0.1787 Elapsed time 13.67s\n",
            "Epoch 6 Batch 1900 Loss 0.1493 Elapsed time 13.65s\n",
            "Epoch 6 Batch 2000 Loss 0.1201 Elapsed time 13.57s\n",
            "Epoch 6 Batch 2100 Loss 0.1507 Elapsed time 13.60s\n",
            "Epoch 6 Batch 2200 Loss 0.1181 Elapsed time 13.59s\n",
            "Epoch 6 Batch 2300 Loss 0.1283 Elapsed time 13.53s\n",
            "Epoch 6 Batch 2400 Loss 0.1289 Elapsed time 13.78s\n",
            "Epoch 6 Batch 2500 Loss 0.1564 Elapsed time 13.58s\n",
            "Epoch 6 Batch 2600 Loss 0.1280 Elapsed time 13.50s\n",
            "Epoch 6 Batch 2700 Loss 0.1079 Elapsed time 13.60s\n",
            "Epoch 6 Batch 2800 Loss 0.1025 Elapsed time 13.63s\n",
            "Epoch 6 Batch 2900 Loss 0.1382 Elapsed time 13.66s\n",
            "Epoch 6 Batch 3000 Loss 0.1755 Elapsed time 13.75s\n",
            "Epoch 6 Batch 3100 Loss 0.1053 Elapsed time 13.59s\n",
            "Epoch 6 Batch 3200 Loss 0.1479 Elapsed time 13.65s\n",
            "Epoch 6 Batch 3300 Loss 0.1891 Elapsed time 13.90s\n",
            "Epoch 6 Batch 3400 Loss 0.1897 Elapsed time 13.60s\n",
            "Epoch 6 Batch 3500 Loss 0.1185 Elapsed time 13.60s\n",
            "Epoch 6 Batch 3600 Loss 0.1167 Elapsed time 13.73s\n",
            "Epoch 6 Batch 3700 Loss 0.1201 Elapsed time 13.67s\n",
            "Epoch 6 Batch 3800 Loss 0.1029 Elapsed time 13.63s\n",
            "Epoch 6 Batch 3900 Loss 0.1157 Elapsed time 13.58s\n",
            "Epoch 6 Batch 4000 Loss 0.1370 Elapsed time 13.59s\n",
            "Epoch 6 Batch 4100 Loss 0.1445 Elapsed time 13.61s\n",
            "Epoch 6 Batch 4200 Loss 0.1559 Elapsed time 13.74s\n",
            "Epoch 6 Batch 4300 Loss 0.1277 Elapsed time 13.56s\n",
            "Epoch 6 Batch 4400 Loss 0.1257 Elapsed time 13.59s\n",
            "Epoch 6 Batch 4500 Loss 0.1291 Elapsed time 13.64s\n",
            "Epoch 6 Batch 4600 Loss 0.1000 Elapsed time 13.59s\n",
            "Epoch 6 Batch 4700 Loss 0.1258 Elapsed time 13.57s\n",
            "Epoch 6 Batch 4800 Loss 0.1114 Elapsed time 13.72s\n",
            "Epoch 6 Batch 4900 Loss 0.1475 Elapsed time 13.58s\n",
            "Epoch 6 Batch 5000 Loss 0.1280 Elapsed time 13.58s\n",
            "I don t really understand why .\n",
            "[[2, 23, 8, 80, 211, 58, 1]]\n",
            "non capisco davvero il perche . <end>\n",
            "Epoch 7 Batch 0 Loss 0.1318 Elapsed time 4.17s\n",
            "Epoch 7 Batch 100 Loss 0.1008 Elapsed time 13.64s\n",
            "Epoch 7 Batch 200 Loss 0.1283 Elapsed time 13.55s\n",
            "Epoch 7 Batch 300 Loss 0.1663 Elapsed time 13.70s\n",
            "Epoch 7 Batch 400 Loss 0.1028 Elapsed time 13.65s\n",
            "Epoch 7 Batch 500 Loss 0.1228 Elapsed time 13.87s\n",
            "Epoch 7 Batch 600 Loss 0.1287 Elapsed time 13.58s\n",
            "Epoch 7 Batch 700 Loss 0.1474 Elapsed time 13.60s\n",
            "Epoch 7 Batch 800 Loss 0.1298 Elapsed time 13.57s\n",
            "Epoch 7 Batch 900 Loss 0.1439 Elapsed time 13.68s\n",
            "Epoch 7 Batch 1000 Loss 0.1133 Elapsed time 13.62s\n",
            "Epoch 7 Batch 1100 Loss 0.1679 Elapsed time 13.59s\n",
            "Epoch 7 Batch 1200 Loss 0.1211 Elapsed time 13.55s\n",
            "Epoch 7 Batch 1300 Loss 0.1361 Elapsed time 13.57s\n",
            "Epoch 7 Batch 1400 Loss 0.1205 Elapsed time 13.53s\n",
            "Epoch 7 Batch 1500 Loss 0.1588 Elapsed time 13.68s\n",
            "Epoch 7 Batch 1600 Loss 0.1382 Elapsed time 13.60s\n",
            "Epoch 7 Batch 1700 Loss 0.1209 Elapsed time 13.53s\n",
            "Epoch 7 Batch 1800 Loss 0.1715 Elapsed time 13.55s\n",
            "Epoch 7 Batch 1900 Loss 0.1508 Elapsed time 13.52s\n",
            "Epoch 7 Batch 2000 Loss 0.1193 Elapsed time 13.56s\n",
            "Epoch 7 Batch 2100 Loss 0.1400 Elapsed time 13.68s\n",
            "Epoch 7 Batch 2200 Loss 0.1141 Elapsed time 13.56s\n",
            "Epoch 7 Batch 2300 Loss 0.1255 Elapsed time 13.65s\n",
            "Epoch 7 Batch 2400 Loss 0.1249 Elapsed time 13.49s\n",
            "Epoch 7 Batch 2500 Loss 0.1528 Elapsed time 13.50s\n",
            "Epoch 7 Batch 2600 Loss 0.1254 Elapsed time 13.48s\n",
            "Epoch 7 Batch 2700 Loss 0.1063 Elapsed time 13.77s\n",
            "Epoch 7 Batch 2800 Loss 0.1012 Elapsed time 13.81s\n",
            "Epoch 7 Batch 2900 Loss 0.1306 Elapsed time 13.47s\n",
            "Epoch 7 Batch 3000 Loss 0.1699 Elapsed time 13.51s\n",
            "Epoch 7 Batch 3100 Loss 0.1003 Elapsed time 13.53s\n",
            "Epoch 7 Batch 3200 Loss 0.1435 Elapsed time 13.52s\n",
            "Epoch 7 Batch 3300 Loss 0.1859 Elapsed time 13.76s\n",
            "Epoch 7 Batch 3400 Loss 0.1767 Elapsed time 13.53s\n",
            "Epoch 7 Batch 3500 Loss 0.1196 Elapsed time 13.58s\n",
            "Epoch 7 Batch 3600 Loss 0.1132 Elapsed time 13.58s\n",
            "Epoch 7 Batch 3700 Loss 0.1176 Elapsed time 13.55s\n",
            "Epoch 7 Batch 3800 Loss 0.0923 Elapsed time 13.56s\n",
            "Epoch 7 Batch 3900 Loss 0.1123 Elapsed time 13.75s\n",
            "Epoch 7 Batch 4000 Loss 0.1377 Elapsed time 13.53s\n",
            "Epoch 7 Batch 4100 Loss 0.1369 Elapsed time 13.52s\n",
            "Epoch 7 Batch 4200 Loss 0.1522 Elapsed time 13.53s\n",
            "Epoch 7 Batch 4300 Loss 0.1191 Elapsed time 13.55s\n",
            "Epoch 7 Batch 4400 Loss 0.1189 Elapsed time 13.54s\n",
            "Epoch 7 Batch 4500 Loss 0.1282 Elapsed time 13.82s\n",
            "Epoch 7 Batch 4600 Loss 0.0994 Elapsed time 13.62s\n",
            "Epoch 7 Batch 4700 Loss 0.1240 Elapsed time 13.53s\n",
            "Epoch 7 Batch 4800 Loss 0.1073 Elapsed time 13.54s\n",
            "Epoch 7 Batch 4900 Loss 0.1409 Elapsed time 13.57s\n",
            "Epoch 7 Batch 5000 Loss 0.1225 Elapsed time 13.63s\n",
            "Tom said a lot of good things .\n",
            "[[4, 109, 9, 108, 22, 75, 233, 1]]\n",
            "tom delle cose buone . <end>\n",
            "Epoch 8 Batch 0 Loss 0.1290 Elapsed time 4.17s\n",
            "Epoch 8 Batch 100 Loss 0.1012 Elapsed time 13.73s\n",
            "Epoch 8 Batch 200 Loss 0.1288 Elapsed time 13.55s\n",
            "Epoch 8 Batch 300 Loss 0.1581 Elapsed time 13.59s\n",
            "Epoch 8 Batch 400 Loss 0.0985 Elapsed time 13.51s\n",
            "Epoch 8 Batch 500 Loss 0.1201 Elapsed time 13.54s\n",
            "Epoch 8 Batch 600 Loss 0.1259 Elapsed time 13.63s\n",
            "Epoch 8 Batch 700 Loss 0.1460 Elapsed time 13.67s\n",
            "Epoch 8 Batch 800 Loss 0.1240 Elapsed time 13.58s\n",
            "Epoch 8 Batch 900 Loss 0.1442 Elapsed time 13.61s\n",
            "Epoch 8 Batch 1000 Loss 0.1032 Elapsed time 13.63s\n",
            "Epoch 8 Batch 1100 Loss 0.1633 Elapsed time 13.57s\n",
            "Epoch 8 Batch 1200 Loss 0.1217 Elapsed time 13.69s\n",
            "Epoch 8 Batch 1300 Loss 0.1240 Elapsed time 13.69s\n",
            "Epoch 8 Batch 1400 Loss 0.1086 Elapsed time 13.56s\n",
            "Epoch 8 Batch 1500 Loss 0.1510 Elapsed time 13.52s\n",
            "Epoch 8 Batch 1600 Loss 0.1391 Elapsed time 13.57s\n",
            "Epoch 8 Batch 1700 Loss 0.1162 Elapsed time 13.56s\n",
            "Epoch 8 Batch 1800 Loss 0.1628 Elapsed time 13.83s\n",
            "Epoch 8 Batch 1900 Loss 0.1485 Elapsed time 13.63s\n",
            "Epoch 8 Batch 2000 Loss 0.1175 Elapsed time 13.59s\n",
            "Epoch 8 Batch 2100 Loss 0.1404 Elapsed time 13.62s\n",
            "Epoch 8 Batch 2200 Loss 0.1048 Elapsed time 13.60s\n",
            "Epoch 8 Batch 2300 Loss 0.1221 Elapsed time 13.86s\n",
            "Epoch 8 Batch 2400 Loss 0.1220 Elapsed time 13.84s\n",
            "Epoch 8 Batch 2500 Loss 0.1484 Elapsed time 13.56s\n",
            "Epoch 8 Batch 2600 Loss 0.1281 Elapsed time 13.52s\n",
            "Epoch 8 Batch 2700 Loss 0.0993 Elapsed time 13.55s\n",
            "Epoch 8 Batch 2800 Loss 0.1000 Elapsed time 13.55s\n",
            "Epoch 8 Batch 2900 Loss 0.1258 Elapsed time 13.53s\n",
            "Epoch 8 Batch 3000 Loss 0.1671 Elapsed time 13.74s\n",
            "Epoch 8 Batch 3100 Loss 0.0992 Elapsed time 13.60s\n",
            "Epoch 8 Batch 3200 Loss 0.1416 Elapsed time 13.62s\n",
            "Epoch 8 Batch 3300 Loss 0.1804 Elapsed time 13.49s\n",
            "Epoch 8 Batch 3400 Loss 0.1720 Elapsed time 13.56s\n",
            "Epoch 8 Batch 3500 Loss 0.1134 Elapsed time 13.48s\n",
            "Epoch 8 Batch 3600 Loss 0.1130 Elapsed time 13.77s\n",
            "Epoch 8 Batch 3700 Loss 0.1109 Elapsed time 13.53s\n",
            "Epoch 8 Batch 3800 Loss 0.0912 Elapsed time 13.54s\n",
            "Epoch 8 Batch 3900 Loss 0.1102 Elapsed time 13.57s\n",
            "Epoch 8 Batch 4000 Loss 0.1378 Elapsed time 13.67s\n",
            "Epoch 8 Batch 4100 Loss 0.1362 Elapsed time 13.55s\n",
            "Epoch 8 Batch 4200 Loss 0.1558 Elapsed time 13.78s\n",
            "Epoch 8 Batch 4300 Loss 0.1209 Elapsed time 13.58s\n",
            "Epoch 8 Batch 4400 Loss 0.1169 Elapsed time 13.53s\n",
            "Epoch 8 Batch 4500 Loss 0.1302 Elapsed time 13.62s\n",
            "Epoch 8 Batch 4600 Loss 0.0983 Elapsed time 13.88s\n",
            "Epoch 8 Batch 4700 Loss 0.1171 Elapsed time 13.67s\n",
            "Epoch 8 Batch 4800 Loss 0.1051 Elapsed time 13.87s\n",
            "Epoch 8 Batch 4900 Loss 0.1391 Elapsed time 13.83s\n",
            "Epoch 8 Batch 5000 Loss 0.1219 Elapsed time 13.82s\n",
            "I m doing this for you .\n",
            "[[2, 21, 179, 24, 30, 3, 1]]\n",
            "sto facendo questo . <end>\n",
            "Epoch 9 Batch 0 Loss 0.1268 Elapsed time 3.90s\n",
            "Epoch 9 Batch 100 Loss 0.0971 Elapsed time 13.61s\n",
            "Epoch 9 Batch 200 Loss 0.1255 Elapsed time 13.69s\n",
            "Epoch 9 Batch 300 Loss 0.1506 Elapsed time 13.79s\n",
            "Epoch 9 Batch 400 Loss 0.0972 Elapsed time 13.70s\n",
            "Epoch 9 Batch 500 Loss 0.1189 Elapsed time 13.66s\n",
            "Epoch 9 Batch 600 Loss 0.1252 Elapsed time 13.70s\n",
            "Epoch 9 Batch 700 Loss 0.1475 Elapsed time 13.64s\n",
            "Epoch 9 Batch 800 Loss 0.1254 Elapsed time 13.62s\n",
            "Epoch 9 Batch 900 Loss 0.1430 Elapsed time 13.85s\n",
            "Epoch 9 Batch 1000 Loss 0.1054 Elapsed time 13.72s\n",
            "Epoch 9 Batch 1100 Loss 0.1615 Elapsed time 13.59s\n",
            "Epoch 9 Batch 1200 Loss 0.1198 Elapsed time 13.71s\n",
            "Epoch 9 Batch 1300 Loss 0.1276 Elapsed time 13.57s\n",
            "Epoch 9 Batch 1400 Loss 0.1110 Elapsed time 13.76s\n",
            "Epoch 9 Batch 1500 Loss 0.1491 Elapsed time 13.78s\n",
            "Epoch 9 Batch 1600 Loss 0.1374 Elapsed time 13.60s\n",
            "Epoch 9 Batch 1700 Loss 0.1136 Elapsed time 13.66s\n",
            "Epoch 9 Batch 1800 Loss 0.1621 Elapsed time 13.67s\n",
            "Epoch 9 Batch 1900 Loss 0.1417 Elapsed time 13.79s\n",
            "Epoch 9 Batch 2000 Loss 0.1130 Elapsed time 13.61s\n",
            "Epoch 9 Batch 2100 Loss 0.1370 Elapsed time 13.79s\n",
            "Epoch 9 Batch 2200 Loss 0.1125 Elapsed time 13.64s\n",
            "Epoch 9 Batch 2300 Loss 0.1214 Elapsed time 13.65s\n",
            "Epoch 9 Batch 2400 Loss 0.1147 Elapsed time 13.62s\n",
            "Epoch 9 Batch 2500 Loss 0.1513 Elapsed time 13.65s\n",
            "Epoch 9 Batch 2600 Loss 0.1208 Elapsed time 13.71s\n",
            "Epoch 9 Batch 2700 Loss 0.0980 Elapsed time 13.78s\n",
            "Epoch 9 Batch 2800 Loss 0.0921 Elapsed time 13.65s\n",
            "Epoch 9 Batch 2900 Loss 0.1270 Elapsed time 13.62s\n",
            "Epoch 9 Batch 3000 Loss 0.1537 Elapsed time 13.65s\n",
            "Epoch 9 Batch 3100 Loss 0.0985 Elapsed time 13.57s\n",
            "Epoch 9 Batch 3200 Loss 0.1367 Elapsed time 13.71s\n",
            "Epoch 9 Batch 3300 Loss 0.1768 Elapsed time 13.77s\n",
            "Epoch 9 Batch 3400 Loss 0.1675 Elapsed time 13.75s\n",
            "Epoch 9 Batch 3500 Loss 0.1092 Elapsed time 13.62s\n",
            "Epoch 9 Batch 3600 Loss 0.1107 Elapsed time 13.62s\n",
            "Epoch 9 Batch 3700 Loss 0.1127 Elapsed time 13.61s\n",
            "Epoch 9 Batch 3800 Loss 0.0860 Elapsed time 13.58s\n",
            "Epoch 9 Batch 3900 Loss 0.1086 Elapsed time 13.73s\n",
            "Epoch 9 Batch 4000 Loss 0.1291 Elapsed time 13.63s\n",
            "Epoch 9 Batch 4100 Loss 0.1330 Elapsed time 13.84s\n",
            "Epoch 9 Batch 4200 Loss 0.1476 Elapsed time 13.73s\n",
            "Epoch 9 Batch 4300 Loss 0.1127 Elapsed time 13.64s\n",
            "Epoch 9 Batch 4400 Loss 0.1145 Elapsed time 13.70s\n",
            "Epoch 9 Batch 4500 Loss 0.1238 Elapsed time 13.76s\n",
            "Epoch 9 Batch 4600 Loss 0.0960 Elapsed time 13.61s\n",
            "Epoch 9 Batch 4700 Loss 0.1154 Elapsed time 13.61s\n",
            "Epoch 9 Batch 4800 Loss 0.1034 Elapsed time 13.61s\n",
            "Epoch 9 Batch 4900 Loss 0.1365 Elapsed time 13.59s\n",
            "Epoch 9 Batch 5000 Loss 0.1187 Elapsed time 13.69s\n",
            "You re manipulative .\n",
            "[[3, 26, 4187, 1]]\n",
            "e . <end>\n",
            "Epoch 10 Batch 0 Loss 0.1265 Elapsed time 3.88s\n",
            "Epoch 10 Batch 100 Loss 0.0992 Elapsed time 13.69s\n",
            "Epoch 10 Batch 200 Loss 0.1202 Elapsed time 13.53s\n",
            "Epoch 10 Batch 300 Loss 0.1498 Elapsed time 13.64s\n",
            "Epoch 10 Batch 400 Loss 0.0999 Elapsed time 13.65s\n",
            "Epoch 10 Batch 500 Loss 0.1162 Elapsed time 13.63s\n",
            "Epoch 10 Batch 600 Loss 0.1189 Elapsed time 13.79s\n",
            "Epoch 10 Batch 700 Loss 0.1508 Elapsed time 13.62s\n",
            "Epoch 10 Batch 800 Loss 0.1199 Elapsed time 13.58s\n",
            "Epoch 10 Batch 900 Loss 0.1426 Elapsed time 13.62s\n",
            "Epoch 10 Batch 1000 Loss 0.1031 Elapsed time 13.64s\n",
            "Epoch 10 Batch 1100 Loss 0.1536 Elapsed time 13.66s\n",
            "Epoch 10 Batch 1200 Loss 0.1215 Elapsed time 13.81s\n",
            "Epoch 10 Batch 1300 Loss 0.1194 Elapsed time 13.65s\n",
            "Epoch 10 Batch 1400 Loss 0.1053 Elapsed time 13.80s\n",
            "Epoch 10 Batch 1500 Loss 0.1373 Elapsed time 13.64s\n",
            "Epoch 10 Batch 1600 Loss 0.1349 Elapsed time 13.62s\n",
            "Epoch 10 Batch 1700 Loss 0.1084 Elapsed time 13.66s\n",
            "Epoch 10 Batch 1800 Loss 0.1561 Elapsed time 13.77s\n",
            "Epoch 10 Batch 1900 Loss 0.1417 Elapsed time 13.61s\n",
            "Epoch 10 Batch 2000 Loss 0.1094 Elapsed time 13.68s\n",
            "Epoch 10 Batch 2100 Loss 0.1391 Elapsed time 13.58s\n",
            "Epoch 10 Batch 2200 Loss 0.1146 Elapsed time 13.58s\n",
            "Epoch 10 Batch 2300 Loss 0.1159 Elapsed time 13.62s\n",
            "Epoch 10 Batch 2400 Loss 0.1138 Elapsed time 13.75s\n",
            "Epoch 10 Batch 2500 Loss 0.1392 Elapsed time 13.69s\n",
            "Epoch 10 Batch 2600 Loss 0.1169 Elapsed time 13.63s\n",
            "Epoch 10 Batch 2700 Loss 0.0978 Elapsed time 13.70s\n",
            "Epoch 10 Batch 2800 Loss 0.0955 Elapsed time 13.56s\n",
            "Epoch 10 Batch 2900 Loss 0.1259 Elapsed time 13.70s\n",
            "Epoch 10 Batch 3000 Loss 0.1564 Elapsed time 13.74s\n",
            "Epoch 10 Batch 3100 Loss 0.0972 Elapsed time 13.54s\n",
            "Epoch 10 Batch 3200 Loss 0.1340 Elapsed time 13.55s\n",
            "Epoch 10 Batch 3300 Loss 0.1744 Elapsed time 13.52s\n",
            "Epoch 10 Batch 3400 Loss 0.1683 Elapsed time 13.57s\n",
            "Epoch 10 Batch 3500 Loss 0.1115 Elapsed time 13.72s\n",
            "Epoch 10 Batch 3600 Loss 0.1069 Elapsed time 13.76s\n",
            "Epoch 10 Batch 3700 Loss 0.1105 Elapsed time 13.73s\n",
            "Epoch 10 Batch 3800 Loss 0.0857 Elapsed time 13.59s\n",
            "Epoch 10 Batch 3900 Loss 0.1098 Elapsed time 13.56s\n",
            "Epoch 10 Batch 4000 Loss 0.1323 Elapsed time 13.55s\n",
            "Epoch 10 Batch 4100 Loss 0.1313 Elapsed time 13.67s\n",
            "Epoch 10 Batch 4200 Loss 0.1527 Elapsed time 13.62s\n",
            "Epoch 10 Batch 4300 Loss 0.1178 Elapsed time 13.60s\n",
            "Epoch 10 Batch 4400 Loss 0.1145 Elapsed time 13.59s\n",
            "Epoch 10 Batch 4500 Loss 0.1135 Elapsed time 13.53s\n",
            "Epoch 10 Batch 4600 Loss 0.0911 Elapsed time 13.50s\n",
            "Epoch 10 Batch 4700 Loss 0.1153 Elapsed time 13.65s\n",
            "Epoch 10 Batch 4800 Loss 0.1082 Elapsed time 13.60s\n",
            "Epoch 10 Batch 4900 Loss 0.1391 Elapsed time 13.57s\n",
            "Epoch 10 Batch 5000 Loss 0.1162 Elapsed time 13.53s\n",
            "I m still in training .\n",
            "[[2, 21, 93, 17, 3141, 1]]\n",
            "mi . <end>\n",
            "Epoch 11 Batch 0 Loss 0.1268 Elapsed time 3.75s\n",
            "Epoch 11 Batch 100 Loss 0.0955 Elapsed time 13.71s\n",
            "Epoch 11 Batch 200 Loss 0.1217 Elapsed time 13.52s\n",
            "Epoch 11 Batch 300 Loss 0.1494 Elapsed time 13.74s\n",
            "Epoch 11 Batch 400 Loss 0.0933 Elapsed time 13.57s\n",
            "Epoch 11 Batch 500 Loss 0.1177 Elapsed time 13.54s\n",
            "Epoch 11 Batch 600 Loss 0.1173 Elapsed time 13.63s\n",
            "Epoch 11 Batch 700 Loss 0.1446 Elapsed time 13.53s\n",
            "Epoch 11 Batch 800 Loss 0.1186 Elapsed time 13.48s\n",
            "Epoch 11 Batch 900 Loss 0.1401 Elapsed time 13.86s\n",
            "Epoch 11 Batch 1000 Loss 0.1069 Elapsed time 13.61s\n",
            "Epoch 11 Batch 1100 Loss 0.1539 Elapsed time 13.49s\n",
            "Epoch 11 Batch 1200 Loss 0.1142 Elapsed time 13.43s\n",
            "Epoch 11 Batch 1300 Loss 0.1230 Elapsed time 13.59s\n",
            "Epoch 11 Batch 1400 Loss 0.1015 Elapsed time 13.57s\n",
            "Epoch 11 Batch 1500 Loss 0.1451 Elapsed time 13.70s\n",
            "Epoch 11 Batch 1600 Loss 0.1369 Elapsed time 13.47s\n",
            "Epoch 11 Batch 1700 Loss 0.1032 Elapsed time 13.52s\n",
            "Epoch 11 Batch 1800 Loss 0.1492 Elapsed time 13.50s\n",
            "Epoch 11 Batch 1900 Loss 0.1418 Elapsed time 13.52s\n",
            "Epoch 11 Batch 2000 Loss 0.1057 Elapsed time 13.51s\n",
            "Epoch 11 Batch 2100 Loss 0.1355 Elapsed time 13.73s\n",
            "Epoch 11 Batch 2200 Loss 0.1052 Elapsed time 13.56s\n",
            "Epoch 11 Batch 2300 Loss 0.1108 Elapsed time 13.60s\n",
            "Epoch 11 Batch 2400 Loss 0.1115 Elapsed time 13.65s\n",
            "Epoch 11 Batch 2500 Loss 0.1400 Elapsed time 13.54s\n",
            "Epoch 11 Batch 2600 Loss 0.1181 Elapsed time 13.54s\n",
            "Epoch 11 Batch 2700 Loss 0.1001 Elapsed time 13.70s\n",
            "Epoch 11 Batch 2800 Loss 0.0885 Elapsed time 13.52s\n",
            "Epoch 11 Batch 2900 Loss 0.1270 Elapsed time 13.56s\n",
            "Epoch 11 Batch 3000 Loss 0.1488 Elapsed time 13.56s\n",
            "Epoch 11 Batch 3100 Loss 0.0969 Elapsed time 13.54s\n",
            "Epoch 11 Batch 3200 Loss 0.1377 Elapsed time 13.87s\n",
            "Epoch 11 Batch 3300 Loss 0.1685 Elapsed time 13.71s\n",
            "Epoch 11 Batch 3400 Loss 0.1607 Elapsed time 13.51s\n",
            "Epoch 11 Batch 3500 Loss 0.1067 Elapsed time 13.57s\n",
            "Epoch 11 Batch 3600 Loss 0.1090 Elapsed time 13.54s\n",
            "Epoch 11 Batch 3700 Loss 0.1064 Elapsed time 13.61s\n",
            "Epoch 11 Batch 3800 Loss 0.0866 Elapsed time 13.57s\n",
            "Epoch 11 Batch 3900 Loss 0.1022 Elapsed time 13.71s\n",
            "Epoch 11 Batch 4000 Loss 0.1276 Elapsed time 13.56s\n",
            "Epoch 11 Batch 4100 Loss 0.1313 Elapsed time 13.54s\n",
            "Epoch 11 Batch 4200 Loss 0.1456 Elapsed time 13.56s\n",
            "Epoch 11 Batch 4300 Loss 0.1112 Elapsed time 13.52s\n",
            "Epoch 11 Batch 4400 Loss 0.1099 Elapsed time 13.61s\n",
            "Epoch 11 Batch 4500 Loss 0.1166 Elapsed time 13.65s\n",
            "Epoch 11 Batch 4600 Loss 0.0895 Elapsed time 13.71s\n",
            "Epoch 11 Batch 4700 Loss 0.1143 Elapsed time 13.55s\n",
            "Epoch 11 Batch 4800 Loss 0.1011 Elapsed time 13.54s\n",
            "Epoch 11 Batch 4900 Loss 0.1315 Elapsed time 13.55s\n",
            "Epoch 11 Batch 5000 Loss 0.1128 Elapsed time 13.71s\n",
            "That was what I was going to say .\n",
            "[[15, 20, 27, 2, 20, 62, 5, 146, 1]]\n",
            "era . <end>\n",
            "Epoch 12 Batch 0 Loss 0.1255 Elapsed time 3.90s\n",
            "Epoch 12 Batch 100 Loss 0.0980 Elapsed time 13.58s\n",
            "Epoch 12 Batch 200 Loss 0.1163 Elapsed time 13.58s\n",
            "Epoch 12 Batch 300 Loss 0.1464 Elapsed time 13.50s\n",
            "Epoch 12 Batch 400 Loss 0.0964 Elapsed time 13.72s\n",
            "Epoch 12 Batch 500 Loss 0.1082 Elapsed time 13.74s\n",
            "Epoch 12 Batch 600 Loss 0.1161 Elapsed time 13.76s\n",
            "Epoch 12 Batch 700 Loss 0.1401 Elapsed time 13.60s\n",
            "Epoch 12 Batch 800 Loss 0.1152 Elapsed time 13.50s\n",
            "Epoch 12 Batch 900 Loss 0.1320 Elapsed time 13.56s\n",
            "Epoch 12 Batch 1000 Loss 0.1023 Elapsed time 13.56s\n",
            "Epoch 12 Batch 1100 Loss 0.1534 Elapsed time 13.58s\n",
            "Epoch 12 Batch 1200 Loss 0.1162 Elapsed time 13.79s\n",
            "Epoch 12 Batch 1300 Loss 0.1166 Elapsed time 13.49s\n",
            "Epoch 12 Batch 1400 Loss 0.1020 Elapsed time 13.58s\n",
            "Epoch 12 Batch 1500 Loss 0.1348 Elapsed time 13.60s\n",
            "Epoch 12 Batch 1600 Loss 0.1322 Elapsed time 13.61s\n",
            "Epoch 12 Batch 1700 Loss 0.1022 Elapsed time 13.55s\n",
            "Epoch 12 Batch 1800 Loss 0.1446 Elapsed time 13.84s\n",
            "Epoch 12 Batch 1900 Loss 0.1357 Elapsed time 13.60s\n",
            "Epoch 12 Batch 2000 Loss 0.1077 Elapsed time 13.59s\n",
            "Epoch 12 Batch 2100 Loss 0.1307 Elapsed time 13.52s\n",
            "Epoch 12 Batch 2200 Loss 0.1085 Elapsed time 13.61s\n",
            "Epoch 12 Batch 2300 Loss 0.1099 Elapsed time 13.56s\n",
            "Epoch 12 Batch 2400 Loss 0.1102 Elapsed time 13.79s\n",
            "Epoch 12 Batch 2500 Loss 0.1420 Elapsed time 13.54s\n",
            "Epoch 12 Batch 2600 Loss 0.1149 Elapsed time 13.60s\n",
            "Epoch 12 Batch 2700 Loss 0.0972 Elapsed time 13.83s\n",
            "Epoch 12 Batch 2800 Loss 0.0867 Elapsed time 13.63s\n",
            "Epoch 12 Batch 2900 Loss 0.1183 Elapsed time 13.65s\n",
            "Epoch 12 Batch 3000 Loss 0.1515 Elapsed time 13.76s\n",
            "Epoch 12 Batch 3100 Loss 0.0932 Elapsed time 13.57s\n",
            "Epoch 12 Batch 3200 Loss 0.1363 Elapsed time 13.60s\n",
            "Epoch 12 Batch 3300 Loss 0.1634 Elapsed time 13.60s\n",
            "Epoch 12 Batch 3400 Loss 0.1576 Elapsed time 13.50s\n",
            "Epoch 12 Batch 3500 Loss 0.1049 Elapsed time 13.59s\n",
            "Epoch 12 Batch 3600 Loss 0.1001 Elapsed time 13.72s\n",
            "Epoch 12 Batch 3700 Loss 0.1016 Elapsed time 13.53s\n",
            "Epoch 12 Batch 3800 Loss 0.0807 Elapsed time 13.58s\n",
            "Epoch 12 Batch 3900 Loss 0.1034 Elapsed time 13.58s\n",
            "Epoch 12 Batch 4000 Loss 0.1189 Elapsed time 13.74s\n",
            "Epoch 12 Batch 4100 Loss 0.1261 Elapsed time 13.73s\n",
            "Epoch 12 Batch 4200 Loss 0.1456 Elapsed time 13.67s\n",
            "Epoch 12 Batch 4300 Loss 0.1106 Elapsed time 13.58s\n",
            "Epoch 12 Batch 4400 Loss 0.1126 Elapsed time 13.52s\n",
            "Epoch 12 Batch 4500 Loss 0.1131 Elapsed time 13.62s\n",
            "Epoch 12 Batch 4600 Loss 0.0863 Elapsed time 13.64s\n",
            "Epoch 12 Batch 4700 Loss 0.1104 Elapsed time 13.62s\n",
            "Epoch 12 Batch 4800 Loss 0.1028 Elapsed time 13.72s\n",
            "Epoch 12 Batch 4900 Loss 0.1310 Elapsed time 13.61s\n",
            "Epoch 12 Batch 5000 Loss 0.1094 Elapsed time 13.92s\n",
            "Fossil fuels won t be available forever .\n",
            "[[6762, 8632, 88, 8, 28, 1428, 1321, 1]]\n",
            "i non saranno disponibile per sempre . <end>\n",
            "Epoch 13 Batch 0 Loss 0.1268 Elapsed time 4.12s\n",
            "Epoch 13 Batch 100 Loss 0.0973 Elapsed time 13.65s\n",
            "Epoch 13 Batch 200 Loss 0.1145 Elapsed time 13.55s\n",
            "Epoch 13 Batch 300 Loss 0.1492 Elapsed time 13.81s\n",
            "Epoch 13 Batch 400 Loss 0.0920 Elapsed time 13.56s\n",
            "Epoch 13 Batch 500 Loss 0.1063 Elapsed time 13.59s\n",
            "Epoch 13 Batch 600 Loss 0.1169 Elapsed time 13.57s\n",
            "Epoch 13 Batch 700 Loss 0.1484 Elapsed time 13.65s\n",
            "Epoch 13 Batch 800 Loss 0.1144 Elapsed time 13.55s\n",
            "Epoch 13 Batch 900 Loss 0.1360 Elapsed time 13.76s\n",
            "Epoch 13 Batch 1000 Loss 0.1008 Elapsed time 13.54s\n",
            "Epoch 13 Batch 1100 Loss 0.1475 Elapsed time 13.65s\n",
            "Epoch 13 Batch 1200 Loss 0.1102 Elapsed time 13.67s\n",
            "Epoch 13 Batch 1300 Loss 0.1213 Elapsed time 13.71s\n",
            "Epoch 13 Batch 1400 Loss 0.1024 Elapsed time 13.63s\n",
            "Epoch 13 Batch 1500 Loss 0.1362 Elapsed time 13.76s\n",
            "Epoch 13 Batch 1600 Loss 0.1295 Elapsed time 13.67s\n",
            "Epoch 13 Batch 1700 Loss 0.1004 Elapsed time 13.65s\n",
            "Epoch 13 Batch 1800 Loss 0.1499 Elapsed time 13.65s\n",
            "Epoch 13 Batch 1900 Loss 0.1360 Elapsed time 13.59s\n",
            "Epoch 13 Batch 2000 Loss 0.1055 Elapsed time 13.64s\n",
            "Epoch 13 Batch 2100 Loss 0.1300 Elapsed time 13.76s\n",
            "Epoch 13 Batch 2200 Loss 0.1058 Elapsed time 13.81s\n",
            "Epoch 13 Batch 2300 Loss 0.1104 Elapsed time 13.76s\n",
            "Epoch 13 Batch 2400 Loss 0.1094 Elapsed time 13.66s\n",
            "Epoch 13 Batch 2500 Loss 0.1355 Elapsed time 13.69s\n",
            "Epoch 13 Batch 2600 Loss 0.1153 Elapsed time 13.63s\n",
            "Epoch 13 Batch 2700 Loss 0.0975 Elapsed time 13.76s\n",
            "Epoch 13 Batch 2800 Loss 0.0863 Elapsed time 13.66s\n",
            "Epoch 13 Batch 2900 Loss 0.1139 Elapsed time 13.61s\n",
            "Epoch 13 Batch 3000 Loss 0.1467 Elapsed time 13.58s\n",
            "Epoch 13 Batch 3100 Loss 0.0937 Elapsed time 13.67s\n",
            "Epoch 13 Batch 3200 Loss 0.1346 Elapsed time 13.67s\n",
            "Epoch 13 Batch 3300 Loss 0.1619 Elapsed time 13.72s\n",
            "Epoch 13 Batch 3400 Loss 0.1530 Elapsed time 13.64s\n",
            "Epoch 13 Batch 3500 Loss 0.1022 Elapsed time 13.73s\n",
            "Epoch 13 Batch 3600 Loss 0.1002 Elapsed time 13.69s\n",
            "Epoch 13 Batch 3700 Loss 0.0996 Elapsed time 13.64s\n",
            "Epoch 13 Batch 3800 Loss 0.0821 Elapsed time 13.72s\n",
            "Epoch 13 Batch 3900 Loss 0.0986 Elapsed time 13.68s\n",
            "Epoch 13 Batch 4000 Loss 0.1197 Elapsed time 13.68s\n",
            "Epoch 13 Batch 4100 Loss 0.1252 Elapsed time 13.61s\n",
            "Epoch 13 Batch 4200 Loss 0.1425 Elapsed time 13.65s\n",
            "Epoch 13 Batch 4300 Loss 0.1106 Elapsed time 13.67s\n",
            "Epoch 13 Batch 4400 Loss 0.1079 Elapsed time 13.78s\n",
            "Epoch 13 Batch 4500 Loss 0.1087 Elapsed time 13.94s\n",
            "Epoch 13 Batch 4600 Loss 0.0881 Elapsed time 13.71s\n",
            "Epoch 13 Batch 4700 Loss 0.1079 Elapsed time 13.68s\n",
            "Epoch 13 Batch 4800 Loss 0.1006 Elapsed time 13.65s\n",
            "Epoch 13 Batch 4900 Loss 0.1325 Elapsed time 13.67s\n",
            "Epoch 13 Batch 5000 Loss 0.1126 Elapsed time 13.80s\n",
            "Give the television remote control back to me .\n",
            "[[117, 7, 1126, 3247, 873, 114, 5, 19, 1]]\n",
            "mi il televisore . <end>\n",
            "Epoch 14 Batch 0 Loss 0.1226 Elapsed time 3.94s\n",
            "Epoch 14 Batch 100 Loss 0.0916 Elapsed time 13.61s\n",
            "Epoch 14 Batch 200 Loss 0.1126 Elapsed time 13.61s\n",
            "Epoch 14 Batch 300 Loss 0.1470 Elapsed time 13.61s\n",
            "Epoch 14 Batch 400 Loss 0.0937 Elapsed time 13.59s\n",
            "Epoch 14 Batch 500 Loss 0.1083 Elapsed time 13.68s\n",
            "Epoch 14 Batch 600 Loss 0.1110 Elapsed time 13.79s\n",
            "Epoch 14 Batch 700 Loss 0.1414 Elapsed time 13.71s\n",
            "Epoch 14 Batch 800 Loss 0.1122 Elapsed time 13.66s\n",
            "Epoch 14 Batch 900 Loss 0.1294 Elapsed time 13.66s\n",
            "Epoch 14 Batch 1000 Loss 0.0960 Elapsed time 13.63s\n",
            "Epoch 14 Batch 1100 Loss 0.1441 Elapsed time 13.62s\n",
            "Epoch 14 Batch 1200 Loss 0.1106 Elapsed time 13.78s\n",
            "Epoch 14 Batch 1300 Loss 0.1149 Elapsed time 13.61s\n",
            "Epoch 14 Batch 1400 Loss 0.0967 Elapsed time 13.63s\n",
            "Epoch 14 Batch 1500 Loss 0.1320 Elapsed time 13.64s\n",
            "Epoch 14 Batch 1600 Loss 0.1290 Elapsed time 13.64s\n",
            "Epoch 14 Batch 1700 Loss 0.1003 Elapsed time 13.74s\n",
            "Epoch 14 Batch 1800 Loss 0.1393 Elapsed time 13.90s\n",
            "Epoch 14 Batch 1900 Loss 0.1340 Elapsed time 13.61s\n",
            "Epoch 14 Batch 2000 Loss 0.0986 Elapsed time 13.61s\n",
            "Epoch 14 Batch 2100 Loss 0.1290 Elapsed time 13.60s\n",
            "Epoch 14 Batch 2200 Loss 0.0993 Elapsed time 13.58s\n",
            "Epoch 14 Batch 2300 Loss 0.1026 Elapsed time 13.67s\n",
            "Epoch 14 Batch 2400 Loss 0.1065 Elapsed time 13.66s\n",
            "Epoch 14 Batch 2500 Loss 0.1294 Elapsed time 13.59s\n",
            "Epoch 14 Batch 2600 Loss 0.1101 Elapsed time 13.64s\n",
            "Epoch 14 Batch 2700 Loss 0.0949 Elapsed time 13.65s\n",
            "Epoch 14 Batch 2800 Loss 0.0864 Elapsed time 13.66s\n",
            "Epoch 14 Batch 2900 Loss 0.1149 Elapsed time 13.76s\n",
            "Epoch 14 Batch 3000 Loss 0.1430 Elapsed time 13.66s\n",
            "Epoch 14 Batch 3100 Loss 0.0912 Elapsed time 13.64s\n",
            "Epoch 14 Batch 3200 Loss 0.1390 Elapsed time 13.64s\n",
            "Epoch 14 Batch 3300 Loss 0.1604 Elapsed time 13.63s\n",
            "Epoch 14 Batch 3400 Loss 0.1601 Elapsed time 13.63s\n",
            "Epoch 14 Batch 3500 Loss 0.0990 Elapsed time 13.67s\n",
            "Epoch 14 Batch 3600 Loss 0.1064 Elapsed time 13.64s\n",
            "Epoch 14 Batch 3700 Loss 0.0999 Elapsed time 13.59s\n",
            "Epoch 14 Batch 3800 Loss 0.0780 Elapsed time 13.63s\n",
            "Epoch 14 Batch 3900 Loss 0.0983 Elapsed time 13.63s\n",
            "Epoch 14 Batch 4000 Loss 0.1171 Elapsed time 13.79s\n",
            "Epoch 14 Batch 4100 Loss 0.1223 Elapsed time 13.86s\n",
            "Epoch 14 Batch 4200 Loss 0.1393 Elapsed time 13.57s\n",
            "Epoch 14 Batch 4300 Loss 0.1128 Elapsed time 13.56s\n",
            "Epoch 14 Batch 4400 Loss 0.1042 Elapsed time 13.61s\n",
            "Epoch 14 Batch 4500 Loss 0.1087 Elapsed time 13.59s\n",
            "Epoch 14 Batch 4600 Loss 0.0854 Elapsed time 13.58s\n",
            "Epoch 14 Batch 4700 Loss 0.1036 Elapsed time 13.74s\n",
            "Epoch 14 Batch 4800 Loss 0.0963 Elapsed time 13.58s\n",
            "Epoch 14 Batch 4900 Loss 0.1263 Elapsed time 13.62s\n",
            "Epoch 14 Batch 5000 Loss 0.1039 Elapsed time 13.61s\n",
            "Take away this box .\n",
            "[[129, 269, 24, 655, 1]]\n",
            "portate via questa scatola . <end>\n",
            "Epoch 15 Batch 0 Loss 0.1213 Elapsed time 3.97s\n",
            "Epoch 15 Batch 100 Loss 0.0935 Elapsed time 13.68s\n",
            "Epoch 15 Batch 200 Loss 0.1109 Elapsed time 13.64s\n",
            "Epoch 15 Batch 300 Loss 0.1397 Elapsed time 13.78s\n",
            "Epoch 15 Batch 400 Loss 0.0907 Elapsed time 13.62s\n",
            "Epoch 15 Batch 500 Loss 0.1124 Elapsed time 13.61s\n",
            "Epoch 15 Batch 600 Loss 0.1115 Elapsed time 13.54s\n",
            "Epoch 15 Batch 700 Loss 0.1453 Elapsed time 13.56s\n",
            "Epoch 15 Batch 800 Loss 0.1063 Elapsed time 13.70s\n",
            "Epoch 15 Batch 900 Loss 0.1329 Elapsed time 13.72s\n",
            "Epoch 15 Batch 1000 Loss 0.0982 Elapsed time 13.59s\n",
            "Epoch 15 Batch 1100 Loss 0.1465 Elapsed time 13.58s\n",
            "Epoch 15 Batch 1200 Loss 0.1119 Elapsed time 13.60s\n",
            "Epoch 15 Batch 1300 Loss 0.1164 Elapsed time 13.83s\n",
            "Epoch 15 Batch 1400 Loss 0.0976 Elapsed time 13.65s\n",
            "Epoch 15 Batch 1500 Loss 0.1306 Elapsed time 13.64s\n",
            "Epoch 15 Batch 1600 Loss 0.1323 Elapsed time 13.60s\n",
            "Epoch 15 Batch 1700 Loss 0.0998 Elapsed time 13.54s\n",
            "Epoch 15 Batch 1800 Loss 0.1425 Elapsed time 13.58s\n",
            "Epoch 15 Batch 1900 Loss 0.1334 Elapsed time 13.60s\n",
            "Epoch 15 Batch 2000 Loss 0.1032 Elapsed time 13.73s\n",
            "Epoch 15 Batch 2100 Loss 0.1285 Elapsed time 13.65s\n",
            "Epoch 15 Batch 2200 Loss 0.1006 Elapsed time 13.58s\n",
            "Epoch 15 Batch 2300 Loss 0.1054 Elapsed time 13.59s\n",
            "Epoch 15 Batch 2400 Loss 0.1056 Elapsed time 13.60s\n",
            "Epoch 15 Batch 2500 Loss 0.1311 Elapsed time 13.61s\n",
            "Epoch 15 Batch 2600 Loss 0.1076 Elapsed time 13.69s\n",
            "Epoch 15 Batch 2700 Loss 0.0955 Elapsed time 13.56s\n",
            "Epoch 15 Batch 2800 Loss 0.0842 Elapsed time 13.55s\n",
            "Epoch 15 Batch 2900 Loss 0.1139 Elapsed time 13.64s\n",
            "Epoch 15 Batch 3000 Loss 0.1423 Elapsed time 13.64s\n",
            "Epoch 15 Batch 3100 Loss 0.0889 Elapsed time 13.56s\n",
            "Epoch 15 Batch 3200 Loss 0.1354 Elapsed time 13.74s\n",
            "Epoch 15 Batch 3300 Loss 0.1577 Elapsed time 13.61s\n",
            "Epoch 15 Batch 3400 Loss 0.1566 Elapsed time 13.60s\n",
            "Epoch 15 Batch 3500 Loss 0.0984 Elapsed time 13.70s\n",
            "Epoch 15 Batch 3600 Loss 0.0991 Elapsed time 13.80s\n",
            "Epoch 15 Batch 3700 Loss 0.0954 Elapsed time 13.65s\n",
            "Epoch 15 Batch 3800 Loss 0.0759 Elapsed time 13.77s\n",
            "Epoch 15 Batch 3900 Loss 0.0953 Elapsed time 13.67s\n",
            "Epoch 15 Batch 4000 Loss 0.1122 Elapsed time 13.55s\n",
            "Epoch 15 Batch 4100 Loss 0.1190 Elapsed time 13.52s\n",
            "Epoch 15 Batch 4200 Loss 0.1402 Elapsed time 13.51s\n",
            "Epoch 15 Batch 4300 Loss 0.1097 Elapsed time 13.53s\n",
            "Epoch 15 Batch 4400 Loss 0.1058 Elapsed time 13.74s\n",
            "Epoch 15 Batch 4500 Loss 0.1109 Elapsed time 13.56s\n",
            "Epoch 15 Batch 4600 Loss 0.0888 Elapsed time 13.71s\n",
            "Epoch 15 Batch 4700 Loss 0.1038 Elapsed time 13.57s\n",
            "Epoch 15 Batch 4800 Loss 0.0943 Elapsed time 13.56s\n",
            "Epoch 15 Batch 4900 Loss 0.1244 Elapsed time 13.60s\n",
            "Epoch 15 Batch 5000 Loss 0.1082 Elapsed time 13.78s\n",
            "Whose fault is it ?\n",
            "[[885, 841, 10, 11, 6]]\n",
            "di chi e ? <end>\n",
            "Epoch 16 Batch 0 Loss 0.1204 Elapsed time 3.88s\n",
            "Epoch 16 Batch 100 Loss 0.0899 Elapsed time 13.58s\n",
            "Epoch 16 Batch 200 Loss 0.1091 Elapsed time 13.56s\n",
            "Epoch 16 Batch 300 Loss 0.1416 Elapsed time 13.60s\n",
            "Epoch 16 Batch 400 Loss 0.0851 Elapsed time 13.50s\n",
            "Epoch 16 Batch 500 Loss 0.1037 Elapsed time 13.57s\n",
            "Epoch 16 Batch 600 Loss 0.1044 Elapsed time 13.68s\n",
            "Epoch 16 Batch 700 Loss 0.1393 Elapsed time 13.60s\n",
            "Epoch 16 Batch 800 Loss 0.1102 Elapsed time 13.90s\n",
            "Epoch 16 Batch 900 Loss 0.1327 Elapsed time 13.53s\n",
            "Epoch 16 Batch 1000 Loss 0.0976 Elapsed time 13.59s\n",
            "Epoch 16 Batch 1100 Loss 0.1499 Elapsed time 13.60s\n",
            "Epoch 16 Batch 1200 Loss 0.1080 Elapsed time 13.77s\n",
            "Epoch 16 Batch 1300 Loss 0.1124 Elapsed time 13.55s\n",
            "Epoch 16 Batch 1400 Loss 0.0936 Elapsed time 13.56s\n",
            "Epoch 16 Batch 1500 Loss 0.1301 Elapsed time 13.57s\n",
            "Epoch 16 Batch 1600 Loss 0.1314 Elapsed time 13.58s\n",
            "Epoch 16 Batch 1700 Loss 0.1057 Elapsed time 13.66s\n",
            "Epoch 16 Batch 1800 Loss 0.1366 Elapsed time 13.83s\n",
            "Epoch 16 Batch 1900 Loss 0.1286 Elapsed time 13.60s\n",
            "Epoch 16 Batch 2000 Loss 0.0983 Elapsed time 13.61s\n",
            "Epoch 16 Batch 2100 Loss 0.1253 Elapsed time 13.59s\n",
            "Epoch 16 Batch 2200 Loss 0.0994 Elapsed time 13.57s\n",
            "Epoch 16 Batch 2300 Loss 0.1024 Elapsed time 13.70s\n",
            "Epoch 16 Batch 2400 Loss 0.1080 Elapsed time 13.64s\n",
            "Epoch 16 Batch 2500 Loss 0.1329 Elapsed time 13.63s\n",
            "Epoch 16 Batch 2600 Loss 0.1116 Elapsed time 13.54s\n",
            "Epoch 16 Batch 2700 Loss 0.0972 Elapsed time 13.56s\n",
            "Epoch 16 Batch 2800 Loss 0.0829 Elapsed time 13.56s\n",
            "Epoch 16 Batch 2900 Loss 0.1138 Elapsed time 13.67s\n",
            "Epoch 16 Batch 3000 Loss 0.1455 Elapsed time 13.63s\n",
            "Epoch 16 Batch 3100 Loss 0.0888 Elapsed time 13.86s\n",
            "Epoch 16 Batch 3200 Loss 0.1324 Elapsed time 13.62s\n",
            "Epoch 16 Batch 3300 Loss 0.1547 Elapsed time 13.61s\n",
            "Epoch 16 Batch 3400 Loss 0.1534 Elapsed time 13.58s\n",
            "Epoch 16 Batch 3500 Loss 0.0956 Elapsed time 13.76s\n",
            "Epoch 16 Batch 3600 Loss 0.0961 Elapsed time 13.64s\n",
            "Epoch 16 Batch 3700 Loss 0.0985 Elapsed time 13.60s\n",
            "Epoch 16 Batch 3800 Loss 0.0760 Elapsed time 13.61s\n",
            "Epoch 16 Batch 3900 Loss 0.0972 Elapsed time 13.61s\n",
            "Epoch 16 Batch 4000 Loss 0.1176 Elapsed time 13.69s\n",
            "Epoch 16 Batch 4100 Loss 0.1205 Elapsed time 13.85s\n",
            "Epoch 16 Batch 4200 Loss 0.1360 Elapsed time 13.62s\n",
            "Epoch 16 Batch 4300 Loss 0.1100 Elapsed time 13.63s\n",
            "Epoch 16 Batch 4400 Loss 0.1058 Elapsed time 13.57s\n",
            "Epoch 16 Batch 4500 Loss 0.1075 Elapsed time 13.67s\n",
            "Epoch 16 Batch 4600 Loss 0.0827 Elapsed time 13.58s\n",
            "Epoch 16 Batch 4700 Loss 0.1017 Elapsed time 13.78s\n",
            "Epoch 16 Batch 4800 Loss 0.0935 Elapsed time 13.69s\n",
            "Epoch 16 Batch 4900 Loss 0.1265 Elapsed time 13.62s\n",
            "Epoch 16 Batch 5000 Loss 0.1044 Elapsed time 13.63s\n",
            "I want two hot dogs with lots of pepper .\n",
            "[[2, 33, 191, 527, 664, 35, 925, 22, 1989, 1]]\n",
            "voglio dei tuo pepe molto caldo . <end>\n",
            "Epoch 17 Batch 0 Loss 0.1166 Elapsed time 4.07s\n",
            "Epoch 17 Batch 100 Loss 0.0919 Elapsed time 13.57s\n",
            "Epoch 17 Batch 200 Loss 0.1114 Elapsed time 13.65s\n",
            "Epoch 17 Batch 300 Loss 0.1458 Elapsed time 13.87s\n",
            "Epoch 17 Batch 400 Loss 0.0877 Elapsed time 13.68s\n",
            "Epoch 17 Batch 500 Loss 0.1069 Elapsed time 13.49s\n",
            "Epoch 17 Batch 600 Loss 0.1045 Elapsed time 13.59s\n",
            "Epoch 17 Batch 700 Loss 0.1413 Elapsed time 13.57s\n",
            "Epoch 17 Batch 800 Loss 0.1005 Elapsed time 13.64s\n",
            "Epoch 17 Batch 900 Loss 0.1301 Elapsed time 13.68s\n",
            "Epoch 17 Batch 1000 Loss 0.0953 Elapsed time 13.58s\n",
            "Epoch 17 Batch 1100 Loss 0.1415 Elapsed time 13.52s\n",
            "Epoch 17 Batch 1200 Loss 0.1027 Elapsed time 13.56s\n",
            "Epoch 17 Batch 1300 Loss 0.1166 Elapsed time 13.67s\n",
            "Epoch 17 Batch 1400 Loss 0.0928 Elapsed time 13.62s\n",
            "Epoch 17 Batch 1500 Loss 0.1333 Elapsed time 13.63s\n",
            "Epoch 17 Batch 1600 Loss 0.1242 Elapsed time 13.55s\n",
            "Epoch 17 Batch 1700 Loss 0.0994 Elapsed time 13.59s\n",
            "Epoch 17 Batch 1800 Loss 0.1411 Elapsed time 13.62s\n",
            "Epoch 17 Batch 1900 Loss 0.1296 Elapsed time 13.63s\n",
            "Epoch 17 Batch 2000 Loss 0.0992 Elapsed time 13.75s\n",
            "Epoch 17 Batch 2100 Loss 0.1300 Elapsed time 13.66s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwddMX_CDEFs",
        "colab_type": "text"
      },
      "source": [
        "### Infer using 20 training examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx-tRr34BY-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sents = (\n",
        "    'What a ridiculous concept!',\n",
        "    'Your idea is not entirely crazy.',\n",
        "    \"A man's worth lies in what he is.\",\n",
        "    'What he did is very wrong.',\n",
        "    \"All three of you need to do that.\",\n",
        "    \"Are you giving me another chance?\",\n",
        "    \"Both Tom and Mary work as models.\",\n",
        "    \"Can I have a few minutes, please?\",\n",
        "    \"Could you close the door, please?\",\n",
        "    \"Did you plant pumpkins this year?\",\n",
        "    \"Do you ever study in the library?\",\n",
        "    \"Don't be deceived by appearances.\",\n",
        "    \"Excuse me. Can you speak English?\",\n",
        "    \"Few people know the true meaning.\",\n",
        "    \"Germany produced many scientists.\",\n",
        "    \"Guess whose birthday it is today.\",\n",
        "    \"He acted like he owned the place.\",\n",
        "    \"Honesty will pay in the long run.\",\n",
        "    \"How do we know this isn't a trap?\",\n",
        "    \"I can't believe you're giving up.\",\n",
        ")\n",
        "\n",
        "for i, test_sent in enumerate(test_sents):\n",
        "    test_sequence = normalize_string(test_sent)\n",
        "    predict(test_sequence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D_J4P-LDKsx",
        "colab_type": "text"
      },
      "source": [
        "### Infer on a random example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMsRRmu7m2wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_alignments, de_bot_alignments, de_mid_alignments, source, prediction = predict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV1_m_YjDPbL",
        "colab_type": "text"
      },
      "source": [
        "### Examine Encoder's attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzfCXDVXm6wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# attention = tf.reduce_mean(de_bot_alignments[0], axis=1).numpy()[0]\n",
        "attention = en_alignments[3][0, 2, :].numpy()\n",
        "print(attention.shape)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.matshow(attention, cmap='jet')\n",
        "ax.set_xticklabels([''] + source, rotation=90)\n",
        "ax.set_yticklabels([''] + source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ia__pcJ4DY-Z",
        "colab_type": "text"
      },
      "source": [
        "### Examine Decoder's lower attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRt7m8PmDdjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention = de_bot_alignments[3][0, 7, :].numpy()\n",
        "print(attention.shape)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.matshow(attention, cmap='jet')\n",
        "ax.set_xticklabels([''] + prediction, rotation=90)\n",
        "ax.set_yticklabels([''] + prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AOVcy73DjWG",
        "colab_type": "text"
      },
      "source": [
        "### Examine Decoder's upper attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbT-_fYsDmLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention = de_mid_alignments[3][0, 2, :].numpy()\n",
        "print(attention.shape)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 10))\n",
        "ax = fig.add_subplot(1, 1, 1)\n",
        "ax.matshow(attention, cmap='jet')\n",
        "ax.set_xticklabels([''] + source, rotation=90)\n",
        "ax.set_yticklabels([''] + prediction)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}